{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# AI supported Time-Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "\n",
    "%matplotlib qt\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "# return the name of the device, which can be cuda (GPU) or cpu\n",
    "print(torch.cuda.get_device_name() if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeuNet Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuNet(object):                                                               # to the class we shall provide a model, a loss_fn and an optimizer.\n",
    "    def __init__(self, model, loss_fn, optimizer):\n",
    "        # Here we define the attributes of our class\n",
    "        \n",
    "        # We start by storing the arguments as attributes \n",
    "        # to use them later\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        # Let's send the model to the specified device right away\n",
    "        self.model.to(self.device)                                                  # here we send the model to the device\n",
    "\n",
    "        # These attributes are defined here, but since they are\n",
    "        # not informed at the moment of creation, we keep them None\n",
    "        self.train_loader = None\n",
    "        self.val_loader = None\n",
    "        self.writer = None\n",
    "        \n",
    "        # These attributes are going to be computed internally\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.total_epochs = 0\n",
    "\n",
    "        # Creates the train_step function for our model, \n",
    "        # loss function and optimizer\n",
    "        # Note: there are NO ARGS there! It makes use of the class\n",
    "        # attributes directly\n",
    "        self.train_step_fn = self._make_train_step_fn()\n",
    "        # Creates the val_step function for our model and loss\n",
    "        self.val_step_fn = self._make_val_step_fn()\n",
    "\n",
    "    def to(self, device):                                                           # this is the function sending the model to the device\n",
    "        # This method allows the user to specify a different device\n",
    "        # It sets the corresponding attribute (to be used later in\n",
    "        # the mini-batches) and sends the model to the device\n",
    "        try:\n",
    "            self.device = device\n",
    "            self.model.to(self.device)\n",
    "        except RuntimeError:\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            print(f\"Couldn't send it to {device}, sending it to {self.device} instead.\")\n",
    "            self.model.to(self.device)\n",
    "\n",
    "    def set_loaders(self, train_loader, val_loader=None):                           # data loaders provide the input data in a sutiable format to the model, in a minibatch size\n",
    "        # This method allows the user to define which train_loader (and val_loader, optionally) to use\n",
    "        # Both loaders are then assigned to attributes of the class\n",
    "        # So they can be referred to later\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "\n",
    "    def set_tensorboard(self, name, folder='runs'):\n",
    "        # This method allows the user to define a SummaryWriter to interface with TensorBoard\n",
    "        suffix = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "        self.writer = SummaryWriter(f'{folder}/{name}_{suffix}')\n",
    "\n",
    "    def _make_train_step_fn(self):\n",
    "        # This method does not need ARGS... it can refer to\n",
    "        # the attributes: self.model, self.loss_fn and self.optimizer\n",
    "        \n",
    "        # Builds function that performs a step in the train loop\n",
    "        def perform_train_step_fn(x, y):\n",
    "            # Sets model to TRAIN mode\n",
    "            self.model.train()                                                      # the model has a different behaviour during training and evaluation mode\n",
    "\n",
    "            # Step 1 - Computes our model's predicted output - forward pass\n",
    "            yhat = self.model(x)\n",
    "            # Step 2 - Computes the loss\n",
    "            loss = self.loss_fn(yhat, y)\n",
    "            # Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n",
    "            loss.backward()\n",
    "            # Step 4 - Updates parameters using gradients and the learning rate\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()                                              # avoid cumulation of gradients\n",
    "\n",
    "            # Returns the loss\n",
    "            return loss.item()\n",
    "\n",
    "        # Returns the function that will be called inside the train loop\n",
    "        return perform_train_step_fn\n",
    "    \n",
    "    def _make_val_step_fn(self):\n",
    "        # Builds function that performs a step in the validation loop\n",
    "        def perform_val_step_fn(x, y):\n",
    "            # Sets model to EVAL mode\n",
    "            self.model.eval()                                                       # here we set the model to evaluation mode\n",
    "\n",
    "            # Step 1 - Computes our model's predicted output - forward pass\n",
    "            yhat = self.model(x)\n",
    "            # Step 2 - Computes the loss\n",
    "            loss = self.loss_fn(yhat, y)\n",
    "            # There is no need to compute Steps 3 and 4, \n",
    "            # since we don't update parameters during evaluation\n",
    "            return loss.item()\n",
    "\n",
    "        return perform_val_step_fn\n",
    "            \n",
    "    def _mini_batch(self, validation=False):\n",
    "        # The mini-batch can be used with both loaders\n",
    "        # The argument `validation`defines which loader and \n",
    "        # corresponding step function is going to be used\n",
    "        if validation:\n",
    "            data_loader = self.val_loader\n",
    "            step_fn = self.val_step_fn\n",
    "        else:\n",
    "            data_loader = self.train_loader\n",
    "            step_fn = self.train_step_fn\n",
    "\n",
    "        if data_loader is None:\n",
    "            return None\n",
    "            \n",
    "        # Once the data loader and step function, this is the \n",
    "        # same mini-batch loop we had before\n",
    "        mini_batch_losses = []\n",
    "        for x_batch, y_batch in data_loader:\n",
    "            x_batch = x_batch.to(self.device)\n",
    "            y_batch = y_batch.to(self.device)\n",
    "\n",
    "            mini_batch_loss = step_fn(x_batch, y_batch)\n",
    "            mini_batch_losses.append(mini_batch_loss)\n",
    "\n",
    "        loss = np.mean(mini_batch_losses)\n",
    "        return loss\n",
    "\n",
    "    def set_seed(self, seed=42):\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False    \n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    def train(self, n_epochs, seed=42):                                             # this function execute the training of the model\n",
    "        # To ensure reproducibility of the training process\n",
    "        self.set_seed(seed)\n",
    "\n",
    "        for epoch in tqdm(range(n_epochs)):\n",
    "            # Keeps track of the numbers of epochs\n",
    "            # by updating the corresponding attribute\n",
    "            self.total_epochs += 1\n",
    "\n",
    "            # inner loop\n",
    "            # Performs training using mini-batches\n",
    "            loss = self._mini_batch(validation=False)\n",
    "            self.losses.append(loss)\n",
    "\n",
    "            # VALIDATION\n",
    "            # no gradients in validation!\n",
    "            with torch.no_grad():\n",
    "                # Performs evaluation using mini-batches\n",
    "                val_loss = self._mini_batch(validation=True)\n",
    "                self.val_losses.append(val_loss)\n",
    "\n",
    "            # If a SummaryWriter has been set...\n",
    "            if self.writer:                                                         # this is optional, i.e. Tensorboard output\n",
    "                scalars = {'training': loss}\n",
    "                if val_loss is not None:\n",
    "                    scalars.update({'validation': val_loss})\n",
    "                # Records both losses for each epoch under the main tag \"loss\"\n",
    "                self.writer.add_scalars(main_tag='loss',\n",
    "                                        tag_scalar_dict=scalars,\n",
    "                                        global_step=epoch)\n",
    "\n",
    "        if self.writer:\n",
    "            # Closes the writer\n",
    "            self.writer.close()\n",
    "\n",
    "    def save_checkpoint(self, filename):\n",
    "        # Builds dictionary with all elements for resuming training\n",
    "        checkpoint = {'epoch': self.total_epochs,\n",
    "                      'model_state_dict': self.model.state_dict(),\n",
    "                      'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                      'loss': self.losses,\n",
    "                      'val_loss': self.val_losses}\n",
    "\n",
    "        torch.save(checkpoint, filename)\n",
    "\n",
    "    def load_checkpoint(self, filename):\n",
    "        # Loads dictionary\n",
    "        checkpoint = torch.load(filename)\n",
    "\n",
    "        # Restore state for model and optimizer\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "        self.total_epochs = checkpoint['epoch']\n",
    "        self.losses = checkpoint['loss']\n",
    "        self.val_losses = checkpoint['val_loss']\n",
    "\n",
    "        self.model.train() # always use TRAIN for resuming training   \n",
    "\n",
    "    def predict(self, x):\n",
    "        # Set is to evaluation mode for predictions\n",
    "        self.model.eval() \n",
    "        # Takes aNumpy input and make it a float tensor\n",
    "        x_tensor = torch.as_tensor(x).float()\n",
    "        # Send input to device and uses model for prediction\n",
    "        y_hat_tensor = self.model(x_tensor.to(self.device))                                 # sending input to device\n",
    "        # Set it back to train mode\n",
    "        self.model.train()\n",
    "        # Detaches it, brings it to CPU and back to Numpy\n",
    "        return y_hat_tensor.detach().cpu().numpy()                                          # sending back to cpu for return\n",
    "\n",
    "    def plot_losses(self):\n",
    "        fig = plt.figure(figsize=(10, 4))\n",
    "        plt.plot(self.losses, label='Training Loss', c='b', lw=1)\n",
    "        plt.plot(self.val_losses, label='Test Loss', c='r', lw=1)\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    def add_graph(self):\n",
    "        # Fetches a single mini-batch so we can use add_graph\n",
    "        if self.train_loader and self.writer:\n",
    "            x_sample, y_sample = next(iter(self.train_loader))\n",
    "            self.writer.add_graph(self.model, x_sample.to(self.device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Transforming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH_O = r\".\\engine_database.csv\"\n",
    "PATH_I = r\".\\network_with_gen.csv\" \n",
    "\n",
    "df = pd.read_csv(PATH_O, sep=\",\", index_col=[0, 1, 2, 3, 4])\n",
    "df_I = pd.read_csv(PATH_I, sep=\";\", decimal=\",\")\n",
    "temp_df = df_I.copy()\n",
    "arr = temp_df.to_numpy()\n",
    "df_I = pd.DataFrame(np.tile(arr, (240, 1)), columns = temp_df.columns)\n",
    "\n",
    "X_df = df.loc[:,:,\"in_service\",\"line\",:,:].stack().unstack(\"id\")\n",
    "number_of_lines = len(set(X_df.columns)) # number of lines\n",
    "df_I.index= X_df.index\n",
    "X_df = pd.concat([X_df, df_I], axis=1)\n",
    "y_df = df.loc[:,:,\"loss_of_load_p_mw\",\"load\",:,:].stack().unstack(\"id\")\n",
    "ysum_df = pd.DataFrame(y_df.sum(axis=1))\n",
    "\n",
    "#%%writefile ./v0.py\n",
    "#from sklearn.model_selection import StratifiedShuffleSplit\n",
    "# loading data from previous script\n",
    "from collections import Counter\n",
    "#X = np.load(\"X.npy\") \n",
    "#y = np.load(\"y.npy\")\n",
    "#number_of_lines = len(set(X_df_t.columns)) # number of lines\n",
    "X = X_df.to_numpy()\n",
    "y = ysum_df.to_numpy()\n",
    "z = pd.DataFrame(X).iloc[:,:number_of_lines].astype(int).astype(str).agg(''.join, axis=1) # labels in str form\n",
    "# sum(axis=1) gives different behaviour, dependent on python version (as it seems)\n",
    "\n",
    "l = [] #  list\n",
    "c = Counter(z)\n",
    "for k in z:\n",
    "    #print(k)\n",
    "    if c[k] == 1:\n",
    "        l.append(str('G0'))\n",
    "    else:\n",
    "        l.append(str(k))\n",
    "\n",
    "\n",
    "# 1104 G0 values        \n",
    "#Split data Model ANN 1\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.1, stratify = pd.DataFrame(l)[0], shuffle=True, random_state=42)\n",
    "    \n",
    "##Split data Model ANN2\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X[(np.array(l) != \"G0\")], y[(np.array(l) != \"G0\")], train_size=0.1, stratify = pd.DataFrame(l)[(np.array(l) != \"G0\")][0], shuffle=True,random_state=42)\n",
    "# X_train = np.append(X_train, X[(np.array(l) == \"G0\")],axis=0)\n",
    "# y_train = np.append(y_train, y[(np.array(l) == \"G0\")], axis=0)\n",
    "\n",
    "#Split data Model ANN 3\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.1,  \n",
    "#                                                 shuffle=True, random_state=42)\n",
    "\n",
    "# standardize data\n",
    "\n",
    "# scaler_x = StandardScaler()\n",
    "# scaler_y = StandardScaler()\n",
    "# X_train = np.concatenate((X_train.T[:number_of_lines].T, scaler_x.fit_transform(X_train.T[number_of_lines:].T)), axis=1).astype(float)\n",
    "# X_val = np.concatenate((X_val.T[:number_of_lines].T, scaler_x.transform(X_val.T[number_of_lines:].T)), axis=1).astype(float)# without fit!\n",
    "# y_train = scaler_y.fit_transform(y_train).astype(float)\n",
    "# y_val = scaler_y.transform(y_val).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>strata</th>\n",
       "      <th>iteration</th>\n",
       "      <th>level_2</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-01 01:00:00</td>\n",
       "      <td>-1.110410e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-01 02:00:00</td>\n",
       "      <td>-1.071424e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-01 03:00:00</td>\n",
       "      <td>-1.041896e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-01 04:00:00</td>\n",
       "      <td>-1.012779e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-01 05:00:00</td>\n",
       "      <td>-8.995495e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-01 06:00:00</td>\n",
       "      <td>1.177491e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-01 07:00:00</td>\n",
       "      <td>1.267960e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-01 08:00:00</td>\n",
       "      <td>1.336059e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-01 09:00:00</td>\n",
       "      <td>1.336210e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-01 10:00:00</td>\n",
       "      <td>1.349950e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-01 11:00:00</td>\n",
       "      <td>1.530109e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-01 12:00:00</td>\n",
       "      <td>1.781677e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-01 13:00:00</td>\n",
       "      <td>-1.308199e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-01 14:00:00</td>\n",
       "      <td>-1.591613e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-01 15:00:00</td>\n",
       "      <td>-1.447300e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-01 16:00:00</td>\n",
       "      <td>-1.508645e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-01 17:00:00</td>\n",
       "      <td>-1.778433e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-01 18:00:00</td>\n",
       "      <td>-1.617197e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-01 19:00:00</td>\n",
       "      <td>-1.470870e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-01 20:00:00</td>\n",
       "      <td>-1.393569e-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    strata  iteration              level_2             0\n",
       "0        0          0  2022-01-01 01:00:00 -1.110410e-06\n",
       "1        0          0  2022-01-01 02:00:00 -1.071424e-06\n",
       "2        0          0  2022-01-01 03:00:00 -1.041896e-06\n",
       "3        0          0  2022-01-01 04:00:00 -1.012779e-06\n",
       "4        0          0  2022-01-01 05:00:00 -8.995495e-07\n",
       "5        0          0  2022-01-01 06:00:00  1.177491e-01\n",
       "6        0          0  2022-01-01 07:00:00  1.267960e-01\n",
       "7        0          0  2022-01-01 08:00:00  1.336059e-01\n",
       "8        0          0  2022-01-01 09:00:00  1.336210e-01\n",
       "9        0          0  2022-01-01 10:00:00  1.349950e-01\n",
       "10       0          0  2022-01-01 11:00:00  1.530109e-01\n",
       "11       0          0  2022-01-01 12:00:00  1.781677e-01\n",
       "12       0          0  2022-01-01 13:00:00 -1.308199e-06\n",
       "13       0          0  2022-01-01 14:00:00 -1.591613e-06\n",
       "14       0          0  2022-01-01 15:00:00 -1.447300e-06\n",
       "15       0          0  2022-01-01 16:00:00 -1.508645e-06\n",
       "16       0          0  2022-01-01 17:00:00 -1.778433e-06\n",
       "17       0          0  2022-01-01 18:00:00 -1.617197e-06\n",
       "18       0          0  2022-01-01 19:00:00 -1.470870e-06\n",
       "19       0          0  2022-01-01 20:00:00 -1.393569e-06"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(ysum_df.reset_index().head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df2 = pd.DataFrame(X_df.reset_index())\n",
    "X_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.248719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61.246691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47.958875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>-0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>-0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>45.466429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>-0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>22.387790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1200 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0\n",
       "0     -0.000002\n",
       "1      1.248719\n",
       "2     61.246691\n",
       "3     -0.000001\n",
       "4     47.958875\n",
       "...         ...\n",
       "1195  -0.000001\n",
       "1196  -0.000002\n",
       "1197  45.466429\n",
       "1198  -0.000001\n",
       "1199  22.387790\n",
       "\n",
       "[1200 rows x 1 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.311727</td>\n",
       "      <td>0.311727</td>\n",
       "      <td>0.311727</td>\n",
       "      <td>0.311727</td>\n",
       "      <td>0.311727</td>\n",
       "      <td>0.311727</td>\n",
       "      <td>0.311727</td>\n",
       "      <td>0.311727</td>\n",
       "      <td>0.311727</td>\n",
       "      <td>...</td>\n",
       "      <td>3.584056</td>\n",
       "      <td>3.584056</td>\n",
       "      <td>3.584056</td>\n",
       "      <td>3.584056</td>\n",
       "      <td>3.584056</td>\n",
       "      <td>5.536304</td>\n",
       "      <td>5.536304</td>\n",
       "      <td>5.536304</td>\n",
       "      <td>8.466359</td>\n",
       "      <td>8.466359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.331466</td>\n",
       "      <td>0.331466</td>\n",
       "      <td>0.331466</td>\n",
       "      <td>0.331466</td>\n",
       "      <td>0.331466</td>\n",
       "      <td>0.331466</td>\n",
       "      <td>0.331466</td>\n",
       "      <td>0.331466</td>\n",
       "      <td>0.331466</td>\n",
       "      <td>...</td>\n",
       "      <td>3.649938</td>\n",
       "      <td>3.649938</td>\n",
       "      <td>3.649938</td>\n",
       "      <td>3.649938</td>\n",
       "      <td>3.649938</td>\n",
       "      <td>4.972258</td>\n",
       "      <td>4.972258</td>\n",
       "      <td>4.972258</td>\n",
       "      <td>7.672092</td>\n",
       "      <td>7.672092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.327682</td>\n",
       "      <td>0.327682</td>\n",
       "      <td>0.327682</td>\n",
       "      <td>0.327682</td>\n",
       "      <td>0.327682</td>\n",
       "      <td>0.327682</td>\n",
       "      <td>0.327682</td>\n",
       "      <td>0.327682</td>\n",
       "      <td>0.327682</td>\n",
       "      <td>...</td>\n",
       "      <td>3.257238</td>\n",
       "      <td>3.257238</td>\n",
       "      <td>3.257238</td>\n",
       "      <td>3.257238</td>\n",
       "      <td>3.257238</td>\n",
       "      <td>4.667912</td>\n",
       "      <td>4.667912</td>\n",
       "      <td>4.667912</td>\n",
       "      <td>7.423841</td>\n",
       "      <td>7.423841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.287097</td>\n",
       "      <td>0.287097</td>\n",
       "      <td>0.287097</td>\n",
       "      <td>0.287097</td>\n",
       "      <td>0.287097</td>\n",
       "      <td>0.287097</td>\n",
       "      <td>0.287097</td>\n",
       "      <td>0.287097</td>\n",
       "      <td>0.287097</td>\n",
       "      <td>...</td>\n",
       "      <td>3.020017</td>\n",
       "      <td>3.020017</td>\n",
       "      <td>3.020017</td>\n",
       "      <td>3.020017</td>\n",
       "      <td>3.020017</td>\n",
       "      <td>4.465290</td>\n",
       "      <td>4.465290</td>\n",
       "      <td>4.465290</td>\n",
       "      <td>7.168182</td>\n",
       "      <td>7.168182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.329431</td>\n",
       "      <td>0.329431</td>\n",
       "      <td>0.329431</td>\n",
       "      <td>0.329431</td>\n",
       "      <td>0.329431</td>\n",
       "      <td>0.329431</td>\n",
       "      <td>0.329431</td>\n",
       "      <td>0.329431</td>\n",
       "      <td>0.329431</td>\n",
       "      <td>...</td>\n",
       "      <td>2.679719</td>\n",
       "      <td>2.679719</td>\n",
       "      <td>2.679719</td>\n",
       "      <td>2.679719</td>\n",
       "      <td>2.679719</td>\n",
       "      <td>4.337053</td>\n",
       "      <td>4.337053</td>\n",
       "      <td>4.337053</td>\n",
       "      <td>7.001256</td>\n",
       "      <td>7.001256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0         1         2         3         4         5         6         7    \\\n",
       "0  1.0  0.311727  0.311727  0.311727  0.311727  0.311727  0.311727  0.311727   \n",
       "1  1.0  0.331466  0.331466  0.331466  0.331466  0.331466  0.331466  0.331466   \n",
       "2  1.0  0.327682  0.327682  0.327682  0.327682  0.327682  0.327682  0.327682   \n",
       "3  1.0  0.287097  0.287097  0.287097  0.287097  0.287097  0.287097  0.287097   \n",
       "4  1.0  0.329431  0.329431  0.329431  0.329431  0.329431  0.329431  0.329431   \n",
       "\n",
       "        8         9    ...       112       113       114       115       116  \\\n",
       "0  0.311727  0.311727  ...  3.584056  3.584056  3.584056  3.584056  3.584056   \n",
       "1  0.331466  0.331466  ...  3.649938  3.649938  3.649938  3.649938  3.649938   \n",
       "2  0.327682  0.327682  ...  3.257238  3.257238  3.257238  3.257238  3.257238   \n",
       "3  0.287097  0.287097  ...  3.020017  3.020017  3.020017  3.020017  3.020017   \n",
       "4  0.329431  0.329431  ...  2.679719  2.679719  2.679719  2.679719  2.679719   \n",
       "\n",
       "        117       118       119       120       121  \n",
       "0  5.536304  5.536304  5.536304  8.466359  8.466359  \n",
       "1  4.972258  4.972258  4.972258  7.672092  7.672092  \n",
       "2  4.667912  4.667912  4.667912  7.423841  7.423841  \n",
       "3  4.465290  4.465290  4.465290  7.168182  7.168182  \n",
       "4  4.337053  4.337053  4.337053  7.001256  7.001256  \n",
       "\n",
       "[5 rows x 122 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X)\n",
    "pd.DataFrame(X[0:5,:114])\n",
    "pd.DataFrame(X[0:5,112:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10800, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%%writefile ./v2.py\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "torch.manual_seed(555)\n",
    "\n",
    "# Builds tensors from numpy arrays BEFORE split\n",
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "X_val_tensor = torch.from_numpy(X_val).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "y_val_tensor = torch.from_numpy(y_val).float()\n",
    "\n",
    "\n",
    "# Builds dataset containing ALL data points\n",
    "\n",
    "dataset_train = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "dataset_val = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = []\n",
    "val_loader = []\n",
    "\n",
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "# # K-fold Cross Validation model evaluation\n",
    "# kfold = KFold(n_splits=5, shuffle=True)\n",
    "# for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset_train)):\n",
    "\n",
    "#     # Print\n",
    "#     print(f'FOLD {fold}')\n",
    "#     print('--------------------------------')\n",
    "\n",
    "#     # Sample elements randomly from a given list of ids, no replacement.\n",
    "#     train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "#     test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "\n",
    "#     # Define data loaders for training and testing data in this fold\n",
    "#     train_loader.append(torch.utils.data.DataLoader(\n",
    "#                       dataset_train, \n",
    "#                       batch_size=256, sampler=train_subsampler))\n",
    "#     val_loader.append(torch.utils.data.DataLoader(\n",
    "#                       dataset_train,\n",
    "#                       batch_size=256, sampler=test_subsampler))\n",
    "\n",
    "   \n",
    "train_loader.append(DataLoader(dataset=dataset_train, batch_size=256, shuffle=True)) #batch_size=256\n",
    "val_loader.append(DataLoader(dataset=dataset_val, batch_size=256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%%writefile ./v3.py\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Sets learning rate - this is \"eta\" ~ the \"n\" like Greek letter\n",
    "lr = 0.001\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# Now we can create a model and send it at once to the device\n",
    "model = nn.Sequential(nn.Linear(X_train.shape[1], 512), nn.ReLU(),\n",
    "                      #nn.Dropout(p=0.1),\n",
    "                      nn.Linear(512, 512), nn.ReLU(),\n",
    "                      nn.Linear(512, 512), nn.ReLU(),\n",
    "                      nn.Linear(512,512), nn.ReLU(),\n",
    "                      nn.Linear(512, 256), nn.ReLU(),\n",
    "                      nn.Linear(256, y_train.shape[1]) ).to(device)\n",
    "\n",
    "# Defines an  optimizer to update the parameters (now retrieved directly from the model)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "neunet = NeuNet(model, loss_fn, optimizer)\n",
    "neunet.set_loaders(train_loader[0], val_loader[0])\n",
    "neunet.set_tensorboard(name=\"runs\", folder = 'machine_learning_tutorial/Test_Grid10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=234, out_features=512, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (9): ReLU()\n",
      "  (10): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(neunet.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:17<00:00,  3.63it/s]\n"
     ]
    }
   ],
   "source": [
    "neunet.train(n_epochs = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "fig = neunet.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.036416869393961375\n",
      "0.00027785422425949944\n"
     ]
    }
   ],
   "source": [
    "print(neunet.val_losses[-1])\n",
    "print(neunet.losses[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.03620007778599569\n",
      "Normalized MSE = 0.00046876930924530304\n",
      "ENSError = 0.01832205329409939\n"
     ]
    }
   ],
   "source": [
    "# prediction of validation points\n",
    "i = 0\n",
    "start_t = 50\n",
    "end_t = start_t + 24*4*5\n",
    "#new_inputs = torch.tensor(X_val).float()\n",
    "new_inputs = np.array(X_val)\n",
    "model.eval()\n",
    "#pred = model(new_inputs.to(device))\n",
    "pred = neunet.predict(new_inputs)\n",
    "orig = y_val\n",
    "f, (ax) = plt.subplots(1,1, figsize=(15, 10)) \n",
    "plt.plot(scaler_y.inverse_transform(orig)[start_t:end_t, i], alpha=.5, linestyle=\"--\", label=\"correct ENS\", lw=1)\n",
    "plt.plot(scaler_y.inverse_transform(pred)[start_t:end_t, i], alpha=.5, linestyle=\"-\", label=\"predicted ENS\", lw=1)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "MSE = np.mean(scaler_y.inverse_transform(orig)- scaler_y.inverse_transform(pred))**2\n",
    "print(f\"MSE = {MSE}\")\n",
    "print(f\"Normalized MSE = {MSE/(scaler_y.inverse_transform(orig).max()-scaler_y.inverse_transform(orig).min())}\")\n",
    "print(f\"ENSError = {abs((scaler_y.inverse_transform(pred).mean() - scaler_y.inverse_transform(orig).mean()))/scaler_y.inverse_transform(orig).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.03376188756954314\n",
      "MSEP = 0.003237335516720473\n",
      "ENSError = 0.017618748181148468\n"
     ]
    }
   ],
   "source": [
    "# prediction of time series sequences\n",
    "new_inputs = np.concatenate((X.T[:number_of_lines].T, scaler_x.transform(X.T[number_of_lines:].T)), axis=1).astype(float)\n",
    "\n",
    "model.eval()\n",
    "#pred = model(new_inputs.to(device))\n",
    "pred = neunet.predict(new_inputs)\n",
    "orig = scaler_y.transform(y)\n",
    "plt.plot(scaler_y.inverse_transform(orig), alpha=.5, linestyle=\"--\", label=\"correct ENS\", lw=1)\n",
    "plt.plot(scaler_y.inverse_transform(pred), alpha=.2, linestyle=\"-\", label=\"predicted ENS\", lw=1)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f\"MSE = {np.mean(scaler_y.inverse_transform(orig)- scaler_y.inverse_transform(pred))**2}\")\n",
    "print(f\"MSEP = {(np.mean(scaler_y.inverse_transform(orig)- scaler_y.inverse_transform(pred))**2)/np.mean(scaler_y.inverse_transform(orig))}\")\n",
    "print(f\"ENSError = {abs((scaler_y.inverse_transform(pred).mean() - scaler_y.inverse_transform(orig).mean()))/scaler_y.inverse_transform(orig).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "ENSp = np.full((int(len(y)/50), 1), np.nan)\n",
    "ENSo = np.full((int(len(y)/50), 1), np.nan)\n",
    "for i, k in enumerate(range(0, len(X), 50), start=0):\n",
    "    #print(k)\n",
    "    ENSp[i] = scaler_y.inverse_transform(pred[k:(k+50)]).sum()\n",
    "    ENSo[i] = scaler_y.inverse_transform(orig[k:(k+50)]).sum()\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1,2, figsize=(15, 10))\n",
    "ax1.set_xlim(min(np.concatenate([ENSo, ENSp])), max(np.concatenate([ENSo, ENSp])))\n",
    "ax1.set_ylim(0, 100)\n",
    "ax2.set_ylim(0, 100)\n",
    "ax2.set_xlim(min(np.concatenate([ENSo, ENSp])), max(np.concatenate([ENSo, ENSp])))\n",
    "#ax1.set_ylim([ymin, ymax])\n",
    "ax1.hist(ENSo, alpha = 0.7)\n",
    "ax1.set_xlabel(\"ENS\")\n",
    "ax1.set_ylabel(\"Density\")\n",
    "ax1.set_title(\"OPF\")\n",
    "ax2.hist(ENSp, alpha = 0.7)\n",
    "ax2.set_xlabel(\"ENS\")\n",
    "ax2.set_ylabel(\"Density\")\n",
    "ax2.set_title(\"ANN\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean ENS predicted = 512.2583982950697\n",
      "mean ENS original = 521.4456054240717\n",
      "median ENS predicted = 378.6937561035156\n",
      "median ENS original = 380.0086413605373\n",
      "standard deviation ENS predicted = 505.9538105755625\n",
      "standard deviation ENS original = 511.4371312030292\n",
      "kurtosis ENS predicted: [-0.54658634]\n",
      "kurtosis ENS original: [-0.58797646]\n",
      "skeweness ENS predicted: [0.76884402]\n",
      "skeweness ENS original: [0.76617784]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import kurtosis, skew\n",
    "print(f\"mean ENS predicted = {ENSp.mean()}\")\n",
    "print(f\"mean ENS original = {ENSo.mean()}\")\n",
    "print(f\"median ENS predicted = {np.median(ENSp)}\")\n",
    "print(f\"median ENS original = {np.median(ENSo)}\")\n",
    "print(f\"standard deviation ENS predicted = {ENSp.std()}\")\n",
    "print(f\"standard deviation ENS original = {ENSo.std()}\")\n",
    "print(f\"kurtosis ENS predicted: {kurtosis(ENSp)}\")\n",
    "print(f\"kurtosis ENS original: {kurtosis(ENSo)}\")\n",
    "print(f\"skeweness ENS predicted: {skew(ENSp)}\")\n",
    "print(f\"skeweness ENS original: {skew(ENSo)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Survivability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MaxLoad = df_I.iloc[:,42:].sum(axis=1).max()\n",
    "#MaxLoad = 173.54 #MW (from analysed time series)\n",
    "crit_load_pu = np.linspace(0,0.45,100)\n",
    "crit_load = crit_load_pu*MaxLoad\n",
    "Sp = []\n",
    "So = []\n",
    "for i, k in enumerate(range(0, len(X), 50), start=0):\n",
    "    #print(k)\n",
    "    Sp.append(scaler_y.inverse_transform(pred[k:(k+50)]).T[0])\n",
    "    So.append(scaler_y.inverse_transform(orig[k:(k+50)]).T[0])\n",
    "Sp = np.array(Sp)\n",
    "So = np.array(So)\n",
    "\n",
    "q = 1 # quantile deciding the maximum loss of load\n",
    "#df.iloc[:,5:][(df.type == \"load\") & (df.field == \"max_p_mw\")].sum(axis=1).max() # max load\n",
    "surv_p = np.full(len(crit_load), np.nan)\n",
    "surv_o = np.full(len(crit_load), np.nan)\n",
    "for i, c in enumerate(crit_load):\n",
    "    #surv_p[i] = (Sp.max(axis=1) < c).sum()/240\n",
    "    surv_p[i] = (np.quantile(Sp, q, axis=1) < c).sum()/240\n",
    "    #surv_o[i] = (So.max(axis=1) < c).sum()/240\n",
    "    surv_o[i] = (np.quantile(So, q, axis=1) < c).sum()/240\n",
    "#plt.plot((1-crit_load_pu), surv)\n",
    "f, (ax1) = plt.subplots(1,1, figsize=(10, 5))\n",
    "ax1.plot((1-crit_load_pu), surv_o*100)\n",
    "ax1.set_xlabel(\"Supplied Load [pu]\")\n",
    "ax1.set_ylabel(\"Survivability [%]\")\n",
    "ax1.set_title(\"Survivability\")\n",
    "ax1.plot((1-crit_load_pu), surv_p*100)\n",
    "ax1.legend([\"OPF\", \"ANN\"], loc =\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sp.max(axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-8.0705933e-02, -5.3177174e-02, -8.9414887e-02, ...,\n",
       "        -7.0711426e-02,  5.3393287e-03,  1.8745128e-02],\n",
       "       [-8.0705933e-02,  2.5886241e-02, -1.1509236e-02, ...,\n",
       "        -7.0711426e-02,  5.3393287e-03,  1.8745128e-02],\n",
       "       [-8.0705933e-02,  8.4565792e+00,  1.1687048e+01, ...,\n",
       "        -7.0711426e-02,  5.3393287e-03,  1.8745128e-02],\n",
       "       ...,\n",
       "       [-8.0705933e-02,  1.5259121e+01,  1.7681065e+01, ...,\n",
       "         7.8249440e+00,  5.4275589e+00,  4.7231231e+00],\n",
       "       [-8.0705933e-02,  4.5227151e+00,  9.9133139e+00, ...,\n",
       "         3.6648519e+00,  1.2816923e+00,  9.7900265e-01],\n",
       "       [-8.0705933e-02,  4.6757789e+00,  1.4335913e+01, ...,\n",
       "         5.5449075e-01,  7.3228902e-01,  5.4297990e-01]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "neunet.save_checkpoint('model_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resuming Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%run -i ./v0.py\n",
    "#%run -i ./v3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[ 0.0975,  0.0945, -0.0405,  ...,  0.0604,  0.0483,  0.0719],\n",
      "        [ 0.0346,  0.0134, -0.0038,  ..., -0.0303, -0.0512, -0.0325],\n",
      "        [ 0.0499,  0.0428,  0.0457,  ...,  0.0619,  0.0425, -0.0277],\n",
      "        ...,\n",
      "        [ 0.0344, -0.0371, -0.0556,  ..., -0.0621,  0.0601, -0.0254],\n",
      "        [ 0.0227,  0.0221, -0.0570,  ...,  0.0116,  0.0552,  0.0548],\n",
      "        [ 0.0075, -0.0497, -0.0140,  ...,  0.0278, -0.0025, -0.0101]])), ('0.bias', tensor([-4.8642e-02, -6.4022e-02,  2.8082e-02, -2.4446e-02,  4.6268e-02,\n",
      "         4.8874e-02,  4.0562e-02,  1.4077e-02, -6.5047e-03, -2.7288e-02,\n",
      "        -3.2381e-02, -5.2258e-02,  5.7791e-02, -1.4139e-02, -2.0281e-02,\n",
      "        -6.7407e-02, -1.7636e-02, -2.2741e-02,  5.3100e-02, -2.0805e-03,\n",
      "         6.1342e-02,  6.9020e-03, -5.3025e-02, -5.0474e-02,  8.7506e-03,\n",
      "        -2.3386e-02, -9.9139e-03, -5.6342e-03, -2.0960e-02,  5.3774e-02,\n",
      "        -3.1210e-02,  2.2367e-02, -5.9626e-02, -3.1538e-05, -2.9136e-02,\n",
      "         5.8115e-03,  2.4725e-02, -3.7035e-02, -5.6828e-03,  5.2912e-04,\n",
      "         5.4402e-02, -5.6243e-02, -4.1138e-03, -6.9213e-02,  2.5951e-02,\n",
      "        -1.7000e-02,  2.1199e-02, -2.5296e-02,  5.3274e-02, -5.0056e-02,\n",
      "         2.5693e-03,  4.6974e-02, -5.0460e-03, -1.7683e-02, -6.0072e-02,\n",
      "        -4.2820e-02, -1.0650e-02, -6.2043e-02,  5.1721e-02, -1.8055e-02,\n",
      "         3.8527e-02,  4.4973e-02, -1.0230e-02, -2.2303e-02, -3.0911e-02,\n",
      "        -7.3005e-02,  1.6897e-02, -4.8383e-02,  3.7639e-02, -2.0988e-02,\n",
      "        -5.3435e-02,  4.7270e-02,  5.3636e-02, -5.4171e-02,  4.4271e-02,\n",
      "        -4.3512e-02,  3.2336e-03, -5.4153e-02, -6.8034e-02, -2.5893e-02,\n",
      "         5.1237e-02, -4.1103e-02, -6.4710e-02,  3.5298e-02, -2.2614e-02,\n",
      "        -4.6217e-02, -1.0791e-02, -3.3625e-02, -5.9675e-02,  1.5499e-02,\n",
      "         2.0383e-02, -3.2027e-03,  3.6331e-02,  3.6816e-03, -6.3648e-02,\n",
      "         1.4258e-02, -4.8522e-02, -2.7051e-02, -3.6488e-02, -3.8826e-02,\n",
      "        -5.1397e-03,  2.5937e-02,  4.9849e-02, -5.5359e-02,  1.8927e-02,\n",
      "        -1.7664e-02, -1.2980e-02,  7.8198e-03,  4.5043e-02, -2.5396e-02,\n",
      "         5.4157e-02,  8.6602e-03, -1.4607e-02,  5.2225e-02, -5.6123e-02,\n",
      "        -4.1338e-02, -2.0248e-02,  3.2728e-02, -2.2108e-02, -5.0172e-02,\n",
      "         3.4735e-02, -5.8321e-02,  7.1731e-03, -4.8331e-02, -7.7569e-02,\n",
      "        -3.7444e-02,  2.2990e-02,  3.8560e-02,  5.4977e-02, -8.3112e-03,\n",
      "        -3.2454e-02, -4.0370e-02, -2.8887e-02, -5.6852e-02,  7.3587e-02,\n",
      "         2.4312e-02, -6.8205e-02, -4.5055e-02,  7.9125e-03,  4.6306e-02,\n",
      "         5.1615e-02, -4.2924e-02, -1.2688e-02,  2.8116e-02, -1.7106e-02,\n",
      "        -6.1883e-02,  5.4930e-02,  5.6030e-02, -5.7942e-02, -9.4925e-03,\n",
      "        -1.4237e-02, -4.3242e-02, -8.6611e-04, -5.6515e-02, -1.1549e-02,\n",
      "         2.9527e-02,  2.3212e-02,  9.0658e-03, -6.1306e-02, -3.4416e-02,\n",
      "        -5.4803e-02, -2.7555e-02, -2.3886e-02, -5.2712e-02, -4.8245e-02,\n",
      "        -3.1079e-02,  6.0649e-02, -5.0808e-03, -5.5965e-02, -3.4043e-02,\n",
      "         6.2548e-02,  3.5105e-02,  2.7381e-02,  1.0989e-02, -2.3912e-03,\n",
      "         9.7919e-03, -7.0966e-02, -2.3756e-02, -3.1724e-02,  4.0793e-02,\n",
      "        -1.9529e-02, -2.6195e-02, -5.8935e-02, -5.7593e-02,  8.1745e-03,\n",
      "         3.8979e-02,  5.8963e-02, -2.2004e-02, -3.7185e-02, -4.4911e-02,\n",
      "         4.6683e-02,  3.7164e-02,  4.7750e-02,  1.7558e-03, -1.8898e-03,\n",
      "         3.6047e-02, -1.3830e-03, -3.0255e-02, -6.1283e-02,  3.8210e-02,\n",
      "         2.6766e-02, -1.1344e-02, -2.1600e-02, -1.7184e-02, -3.2148e-02,\n",
      "         7.7460e-03,  9.6666e-04,  4.7235e-02,  6.3284e-02,  2.0698e-02,\n",
      "        -4.6466e-02, -4.2151e-02,  3.8905e-02, -9.7885e-03, -7.9513e-02,\n",
      "        -4.8735e-02, -6.9717e-02, -1.5542e-03,  2.9656e-02, -4.0109e-02,\n",
      "        -3.4903e-02,  5.3734e-02,  3.6218e-02,  3.2180e-02, -6.8573e-02,\n",
      "         3.4691e-02,  4.3995e-02,  3.5278e-02,  2.6283e-02, -3.7704e-02,\n",
      "        -5.4765e-02,  3.2272e-02,  1.4203e-02,  3.5804e-02, -5.2387e-02,\n",
      "         2.7679e-03,  3.9757e-02,  1.2732e-02, -5.9701e-02, -8.4522e-02,\n",
      "         3.2481e-02, -3.0609e-02, -3.7592e-02, -6.4778e-02, -5.8358e-02,\n",
      "         4.2260e-02,  1.7855e-03,  1.2758e-02,  3.3885e-02,  3.8177e-02,\n",
      "         4.5011e-02, -3.1456e-02, -2.0097e-02,  3.0499e-03, -5.6424e-02,\n",
      "         4.1727e-02,  5.3781e-02, -3.9272e-02, -3.5936e-03, -9.4085e-04,\n",
      "         1.9811e-02,  2.5823e-02, -2.7992e-02,  2.8566e-02, -3.6656e-02,\n",
      "         5.8455e-02, -4.3516e-02,  5.4371e-02,  2.0111e-03, -3.6524e-02,\n",
      "         1.1447e-03, -2.5130e-03,  5.5424e-02,  1.5243e-02, -1.4647e-02,\n",
      "        -6.2558e-02,  5.2878e-02, -1.7253e-02,  6.6544e-02,  6.3773e-02,\n",
      "         3.4152e-02,  1.7312e-02,  2.0517e-03, -6.0314e-03,  2.0282e-02,\n",
      "        -3.8004e-02,  2.6254e-02,  5.0684e-02, -2.8352e-02, -7.3154e-02,\n",
      "         1.4783e-03, -2.5789e-02, -5.3022e-02, -5.1734e-02,  6.4064e-02,\n",
      "        -1.7080e-03, -1.9950e-02,  5.5278e-02,  4.3397e-03, -2.7617e-02,\n",
      "        -1.5173e-02,  3.3073e-02, -2.3875e-02, -5.4338e-02,  5.4431e-02,\n",
      "         7.9233e-03,  5.6654e-03,  7.8059e-03, -4.6121e-02, -2.5353e-02,\n",
      "         4.9661e-02, -4.1910e-02, -9.7961e-03, -3.4516e-02, -5.0641e-02,\n",
      "        -6.7500e-02,  3.1600e-02, -6.8272e-02,  4.4390e-02, -3.2922e-02,\n",
      "        -3.9708e-02,  4.9001e-02, -4.6688e-02,  1.2427e-02,  7.6950e-03,\n",
      "        -3.1724e-02, -6.6560e-02,  1.2663e-02, -4.6840e-02,  1.8247e-02,\n",
      "        -5.3308e-02,  5.6202e-02, -5.0806e-02, -1.1906e-02,  7.9345e-03,\n",
      "        -5.6379e-02, -2.1402e-02,  9.1360e-03,  4.2840e-02,  6.5405e-02,\n",
      "        -1.0616e-02,  5.2906e-04,  2.3657e-06,  3.4616e-02,  6.8193e-02,\n",
      "        -3.2373e-02,  7.7927e-03, -4.7102e-02,  5.1846e-02,  2.1721e-02,\n",
      "        -6.4150e-04, -2.1325e-02, -4.9479e-02, -1.5024e-03, -1.3562e-02,\n",
      "         1.7586e-02, -7.0483e-03, -1.6341e-02, -3.5230e-03, -7.1340e-02,\n",
      "        -2.5567e-02, -4.4780e-02, -1.5440e-02, -5.9330e-02, -7.3357e-02,\n",
      "        -2.1088e-02,  2.5439e-02,  3.1215e-02, -6.9068e-02, -5.8687e-02,\n",
      "        -2.6988e-02, -4.5250e-03,  5.0595e-02, -4.0059e-03, -3.5837e-02,\n",
      "        -1.2599e-02,  2.1638e-02,  3.6709e-02, -4.1495e-02, -5.4213e-02,\n",
      "        -1.9477e-02,  2.0876e-02, -3.9104e-02,  4.8362e-02, -3.4909e-02,\n",
      "         1.3263e-02, -6.7751e-02, -3.8474e-02, -1.1358e-02, -4.3594e-02,\n",
      "         1.4762e-02, -2.0709e-02,  6.5149e-02, -5.4590e-02,  4.6566e-02,\n",
      "        -9.2148e-03, -5.3685e-02,  2.3485e-02, -3.5856e-02,  4.4606e-02,\n",
      "        -2.1712e-02, -6.1369e-02, -5.3222e-02,  9.0684e-03,  4.7714e-02,\n",
      "         3.9031e-03, -5.9590e-02,  2.1937e-02,  7.4894e-03,  1.2538e-02,\n",
      "        -5.1753e-02, -1.0568e-02, -7.7530e-02,  3.5783e-02, -5.1207e-02,\n",
      "        -6.2606e-02,  5.8202e-02,  2.4376e-02,  2.5359e-02,  6.4667e-02,\n",
      "        -1.5275e-02, -4.7548e-02,  3.9731e-02, -4.2998e-02,  3.5641e-02,\n",
      "         5.7292e-02, -2.4876e-02,  1.1032e-03, -2.7467e-02, -3.0414e-02,\n",
      "        -1.7717e-02, -3.3132e-02, -2.2467e-02,  1.0378e-02, -3.4116e-02,\n",
      "         3.3469e-02,  3.3896e-02, -6.0123e-02, -3.0980e-02, -3.4814e-02,\n",
      "        -6.3164e-02,  5.3006e-02, -2.0229e-02,  5.4150e-02,  3.1225e-02,\n",
      "         3.1372e-02,  2.9221e-02, -5.6634e-02,  1.0942e-02, -2.9388e-02,\n",
      "         3.4056e-03, -1.5761e-02, -7.0149e-02,  2.8033e-02,  1.0671e-02,\n",
      "         1.1380e-02, -4.4834e-02, -5.4622e-02, -4.6182e-02, -5.7096e-02,\n",
      "        -5.7427e-02, -1.8222e-02, -5.0520e-02,  2.9502e-02, -7.1734e-02,\n",
      "        -1.0566e-02,  4.3580e-02, -4.1016e-02,  1.4907e-02,  5.1491e-02,\n",
      "        -1.0221e-02, -4.8681e-02,  5.2307e-02, -3.3413e-02,  3.4101e-02,\n",
      "         1.4638e-02, -3.8985e-02,  2.5247e-02, -4.9397e-02, -9.5636e-03,\n",
      "         1.1269e-02, -1.1933e-02,  1.2199e-02, -3.6520e-02, -8.2474e-02,\n",
      "        -1.0298e-02,  3.5593e-02, -1.9851e-02,  8.7575e-03,  1.4413e-02,\n",
      "        -1.3605e-03, -7.1743e-02, -6.2353e-02, -3.1949e-02, -3.0221e-02,\n",
      "         4.3045e-02, -6.1114e-02,  3.9586e-02,  3.9780e-02, -1.5824e-03,\n",
      "         3.7091e-02,  2.3201e-02, -6.9561e-02,  2.2336e-02,  2.1542e-02,\n",
      "         1.2292e-02, -3.4779e-02,  3.9257e-02, -8.5924e-03, -6.3732e-02,\n",
      "         1.3194e-02, -6.0701e-02])), ('2.weight', tensor([[-0.0467,  0.0154, -0.0108,  ..., -0.0116,  0.0273, -0.0093],\n",
      "        [-0.0582, -0.0367, -0.0166,  ..., -0.0260, -0.0162, -0.0534],\n",
      "        [ 0.0008, -0.0354, -0.0082,  ..., -0.0005,  0.0473,  0.0264],\n",
      "        ...,\n",
      "        [ 0.0195, -0.0436, -0.0003,  ..., -0.0055,  0.0121,  0.0023],\n",
      "        [-0.0646, -0.0194,  0.0241,  ..., -0.0558,  0.0291, -0.0187],\n",
      "        [-0.0928, -0.0342, -0.0328,  ..., -0.0465, -0.0445,  0.0338]])), ('2.bias', tensor([-3.2697e-02, -6.8235e-02, -1.3992e-02, -6.2480e-02, -8.3830e-03,\n",
      "         2.3495e-02,  1.6627e-02, -2.6087e-02, -1.1274e-02, -3.8680e-02,\n",
      "         4.5923e-02,  2.8938e-02, -1.9131e-02,  2.1139e-02,  4.7009e-02,\n",
      "        -4.4858e-02,  3.1436e-02,  8.1797e-03, -1.5408e-02, -1.3766e-03,\n",
      "        -3.5889e-02, -6.4315e-02,  5.4178e-02, -2.3289e-03, -1.9193e-02,\n",
      "        -5.7284e-02, -1.0077e-02,  4.5767e-02,  2.7289e-02, -2.0406e-02,\n",
      "        -3.5963e-03, -2.3988e-02, -3.7735e-02, -5.2427e-02,  2.0325e-02,\n",
      "        -2.8330e-02, -4.7711e-02,  1.2042e-02,  3.0137e-02, -4.3491e-02,\n",
      "        -2.7628e-02, -8.0801e-03, -6.2586e-03, -1.1027e-02, -8.0174e-03,\n",
      "        -9.2550e-03,  2.9007e-02, -1.1078e-02, -7.0354e-02,  2.0946e-02,\n",
      "         2.8106e-02,  3.7256e-03, -6.2885e-05, -3.9604e-02, -3.7868e-02,\n",
      "        -5.3404e-02,  1.1531e-02, -2.7671e-02, -3.1881e-02, -4.3119e-03,\n",
      "         1.0483e-03,  1.0395e-02,  7.1215e-03, -1.5253e-02, -2.7459e-02,\n",
      "        -6.7021e-02, -1.3497e-02, -6.0148e-02,  4.8834e-02,  1.2472e-02,\n",
      "         5.1764e-02,  4.4717e-02, -1.0764e-03, -6.1599e-02,  2.3450e-02,\n",
      "        -1.4004e-03, -6.9088e-02, -2.9103e-02,  5.2690e-03, -2.3522e-02,\n",
      "         2.8520e-02,  4.1449e-02,  1.6207e-02, -3.1468e-03,  7.3972e-03,\n",
      "        -3.2111e-02,  3.2255e-03, -3.0692e-02, -1.8224e-02,  8.3117e-03,\n",
      "        -1.5992e-02, -2.9128e-02, -2.1770e-02,  5.3536e-02, -5.4486e-02,\n",
      "         2.2089e-02, -1.3552e-02, -2.4525e-02, -3.1947e-02,  2.7557e-02,\n",
      "        -1.8300e-02, -8.0205e-03, -3.1661e-03,  2.0833e-02,  7.6277e-03,\n",
      "         8.5798e-05, -9.4880e-03, -5.6969e-03,  2.2438e-02, -4.9612e-02,\n",
      "        -1.6423e-02, -3.7413e-02, -2.3648e-02,  2.2182e-02,  1.1625e-02,\n",
      "         7.3036e-03, -1.3317e-02,  2.1622e-03, -5.4788e-03, -3.5196e-02,\n",
      "         1.7532e-02,  3.5754e-02,  2.3786e-02, -1.5152e-02,  7.8752e-03,\n",
      "        -3.9957e-02,  9.7383e-03, -4.0101e-02, -1.8215e-03, -3.5447e-02,\n",
      "        -4.2414e-02,  4.9034e-04,  3.7700e-03,  9.4821e-03, -1.4387e-02,\n",
      "        -3.7766e-02,  9.8579e-03, -3.2587e-02, -3.9242e-02,  2.8729e-02,\n",
      "         3.0664e-02,  6.5137e-03, -1.8806e-02, -2.4924e-02, -1.4181e-02,\n",
      "        -2.8906e-02,  1.6229e-02, -2.0192e-02,  1.3920e-02,  1.5921e-02,\n",
      "        -3.4065e-02,  1.3239e-02,  5.3756e-04, -3.1524e-03,  4.3876e-02,\n",
      "        -6.1507e-03,  6.1911e-03,  2.7418e-02,  2.9885e-02, -1.3887e-03,\n",
      "        -2.7964e-03, -1.0271e-02,  4.2416e-03,  3.3764e-02, -5.4905e-02,\n",
      "         3.3500e-02, -2.3441e-03, -2.2459e-02,  2.1627e-02, -5.0020e-02,\n",
      "         1.8490e-02,  3.9473e-02, -3.4907e-02,  3.1168e-02, -3.3442e-02,\n",
      "        -2.7572e-02,  1.3674e-02, -1.5370e-02, -1.2392e-02, -4.5396e-02,\n",
      "        -2.9191e-02, -5.5716e-02,  1.1282e-02, -3.2459e-02, -5.6017e-02,\n",
      "        -2.6157e-02, -2.9493e-02, -2.1479e-02,  3.4646e-02, -4.0503e-03,\n",
      "        -1.1557e-02,  2.5735e-02,  3.6316e-02,  3.4851e-02, -1.7793e-02,\n",
      "        -4.9240e-02, -3.2419e-02, -4.3779e-02, -2.1338e-02, -1.8803e-02,\n",
      "        -2.6322e-02, -4.4120e-02, -3.0868e-02, -4.3649e-03, -3.1656e-03,\n",
      "        -1.2170e-02,  3.1831e-02,  1.3228e-02,  4.4997e-02, -3.6902e-02,\n",
      "         2.6894e-02, -1.2379e-02,  3.8433e-02, -1.7313e-02, -3.6148e-02,\n",
      "         2.2511e-02, -2.5057e-03, -1.7895e-02, -4.1503e-02, -3.9942e-02,\n",
      "        -1.8160e-03, -2.6989e-02, -4.1242e-02, -2.9215e-02, -2.8418e-02,\n",
      "        -3.1523e-03,  2.5086e-02, -9.3595e-03, -2.8904e-02,  1.8682e-02,\n",
      "        -3.7909e-02, -2.8781e-02, -1.3897e-02, -2.7925e-02, -4.2436e-02,\n",
      "         4.9022e-02,  1.1839e-02,  3.9786e-04, -8.9812e-03,  5.6372e-02,\n",
      "         3.3839e-02, -4.0641e-02,  3.7834e-02, -4.5613e-02, -4.8691e-02,\n",
      "        -2.2422e-02,  4.3084e-02,  2.7976e-02, -2.5304e-02, -5.1466e-02,\n",
      "        -4.4729e-03, -1.0793e-02,  3.7114e-02, -5.4772e-02,  4.0403e-02,\n",
      "         3.2893e-02, -3.5930e-02,  3.8109e-02, -3.2895e-02, -2.6077e-02,\n",
      "        -2.7541e-02, -2.7351e-02, -8.1285e-04, -2.9432e-02,  1.8122e-02,\n",
      "         1.9064e-02, -4.2680e-02, -1.2290e-03,  2.0616e-02, -4.6947e-02,\n",
      "        -4.2650e-02,  2.7686e-02, -1.2635e-02,  1.4488e-02, -8.0727e-03,\n",
      "         3.4260e-02,  9.9979e-03, -1.3727e-02,  2.2922e-03,  6.0369e-03,\n",
      "         5.9670e-02, -5.1076e-03, -4.5336e-02, -5.9754e-02, -3.0517e-02,\n",
      "        -4.2464e-02, -7.1872e-02,  2.2499e-03,  3.1938e-03,  1.0324e-02,\n",
      "        -1.1316e-02, -1.1843e-02, -3.0198e-02,  1.0262e-02, -1.4432e-02,\n",
      "        -6.6637e-03, -1.0187e-02, -4.5328e-02, -1.5360e-02, -3.9335e-02,\n",
      "        -1.7056e-02, -2.2531e-02,  1.8705e-02, -4.6006e-02, -6.0689e-02,\n",
      "         2.9137e-03,  2.4486e-02, -4.2114e-02, -2.4830e-03, -5.2891e-02,\n",
      "        -8.3318e-03,  2.7416e-02, -4.6129e-02, -3.9380e-02, -1.0235e-02,\n",
      "        -4.0867e-02, -2.6436e-02,  2.9964e-02,  3.1704e-03,  2.1421e-02,\n",
      "        -2.0889e-02,  3.4221e-02, -1.0781e-02, -2.2664e-02, -6.1017e-02,\n",
      "        -3.0696e-02, -2.7941e-02, -4.8893e-02,  1.7091e-02, -6.0020e-03,\n",
      "        -3.9835e-02, -2.0492e-02, -2.2849e-02,  1.8593e-02, -3.9362e-02,\n",
      "         2.3700e-02, -4.6391e-03, -5.3185e-03,  1.2966e-02, -2.9805e-02,\n",
      "        -1.0223e-02,  2.2277e-02, -4.3784e-02,  1.9173e-03,  3.7161e-04,\n",
      "         3.0816e-02, -4.3456e-03, -3.7404e-02,  3.7445e-02, -1.8758e-02,\n",
      "         1.3535e-02, -2.0152e-02, -1.1145e-02, -5.8739e-02,  1.5542e-02,\n",
      "         2.4084e-02,  1.9575e-02,  1.6202e-02,  4.9973e-02, -7.0071e-03,\n",
      "        -3.0191e-02, -3.8132e-02, -4.4396e-02, -4.4180e-03, -3.0050e-02,\n",
      "        -3.3862e-02, -4.7201e-02,  7.5045e-03,  3.8330e-02, -3.2235e-02,\n",
      "         3.6641e-05,  9.2408e-03, -1.3027e-02, -4.5728e-02, -5.1296e-02,\n",
      "         5.7702e-02,  2.2304e-02,  2.2785e-02, -2.4622e-02, -2.8324e-02,\n",
      "         7.4239e-03, -7.5109e-03,  2.9951e-02,  1.4235e-02,  3.9695e-02,\n",
      "        -3.2928e-02, -3.3119e-02, -2.3896e-02,  3.8780e-02,  1.4126e-02,\n",
      "        -5.2481e-03,  2.2003e-02, -1.1916e-02, -4.3261e-02,  2.4619e-02,\n",
      "        -4.9242e-02,  2.1084e-02,  1.3856e-02, -2.7857e-02, -2.2570e-02,\n",
      "        -3.2142e-02, -2.7504e-02,  8.7976e-03,  1.1156e-02, -3.4504e-02,\n",
      "        -4.5309e-02,  1.9719e-02, -4.4061e-02, -5.0231e-02,  3.7344e-02,\n",
      "         1.9704e-02, -2.1481e-02, -4.7364e-02, -1.3113e-02, -2.0240e-02,\n",
      "         4.4674e-03, -3.1204e-02,  7.2183e-03, -4.1439e-02, -2.1091e-02,\n",
      "        -2.6877e-02, -8.4217e-03, -3.0170e-03,  2.6329e-02,  1.9694e-02,\n",
      "        -4.3908e-02, -3.3040e-02,  5.0487e-03, -3.2574e-02,  1.7116e-02,\n",
      "         6.2926e-03, -2.4187e-02,  2.1542e-02, -4.6413e-02, -4.3354e-02,\n",
      "        -7.3367e-02,  1.8077e-02, -7.4632e-03,  1.8792e-03,  1.3851e-02,\n",
      "         3.4101e-02,  1.5413e-02,  1.7782e-02,  1.2969e-02,  1.0087e-02,\n",
      "        -1.7763e-03, -4.3309e-02, -2.4202e-02, -1.6117e-02, -2.5968e-02,\n",
      "         1.6622e-02, -1.2862e-02, -3.1764e-04,  2.6044e-02, -3.0907e-02,\n",
      "        -1.9370e-02,  2.3770e-02,  2.9814e-02, -2.1138e-02,  2.1279e-02,\n",
      "        -3.5907e-02,  1.8351e-02, -1.0847e-02, -6.0272e-02,  3.8942e-02,\n",
      "        -2.8716e-03, -2.1176e-02, -2.2023e-02,  1.6335e-02,  4.4041e-02,\n",
      "        -1.3159e-02,  1.4743e-02,  3.2276e-02,  1.3748e-02, -5.6528e-03,\n",
      "        -7.4672e-03,  2.6927e-02,  1.8394e-02,  1.4972e-02,  1.8053e-03,\n",
      "         3.4700e-02, -1.3972e-02,  3.3234e-03,  1.5443e-02, -1.7401e-02,\n",
      "        -1.8149e-02, -5.0599e-02, -3.1348e-02,  4.1075e-02,  2.6803e-02,\n",
      "         2.6070e-02, -3.2397e-02, -2.9797e-02,  1.6585e-02, -1.8460e-02,\n",
      "         1.9925e-03, -2.4648e-02,  5.6343e-03, -3.0020e-02, -1.4953e-02,\n",
      "         1.2867e-02, -1.3229e-02, -8.5949e-03, -2.3932e-02,  5.4372e-03,\n",
      "         2.8579e-03, -2.3148e-02,  2.8078e-02,  1.3592e-02, -2.5256e-02,\n",
      "         3.9180e-02,  1.9725e-02])), ('4.weight', tensor([[ 0.0137, -0.0430, -0.0272,  ..., -0.0181, -0.0026, -0.0185],\n",
      "        [ 0.0056, -0.0071, -0.0439,  ...,  0.0207, -0.0438, -0.0380],\n",
      "        [-0.0026, -0.0476, -0.0580,  ..., -0.0335,  0.0397,  0.0389],\n",
      "        ...,\n",
      "        [ 0.0133, -0.0231,  0.0252,  ..., -0.0117, -0.0290,  0.0237],\n",
      "        [ 0.0006,  0.0348,  0.0012,  ...,  0.0404, -0.0188, -0.0290],\n",
      "        [ 0.0102, -0.0157,  0.0428,  ..., -0.0435, -0.0325, -0.0721]])), ('4.bias', tensor([-4.3124e-02, -6.0502e-02, -9.9909e-03, -2.1852e-02, -1.2212e-02,\n",
      "        -2.9732e-02, -7.6704e-03, -1.2614e-02, -2.3956e-02, -1.9277e-02,\n",
      "         2.9833e-02, -3.0973e-02,  5.5445e-02,  2.1306e-02,  5.6438e-02,\n",
      "        -2.1641e-02,  4.6036e-02, -3.3870e-03,  2.3088e-04,  1.8155e-02,\n",
      "        -2.1638e-02,  1.2251e-03, -3.7239e-02,  7.7374e-03,  6.2584e-02,\n",
      "         1.0854e-02, -1.5807e-02, -4.1262e-02,  2.7087e-02, -1.1643e-02,\n",
      "        -4.7637e-02,  2.0163e-02,  3.3533e-02,  2.4869e-02,  3.4862e-02,\n",
      "        -4.6115e-02,  5.2560e-03, -8.4404e-03,  5.4451e-02, -2.8487e-02,\n",
      "        -8.5966e-03,  1.0529e-02,  3.2453e-02,  2.4697e-02, -5.4063e-02,\n",
      "        -4.0588e-02,  3.4630e-03, -5.3718e-03,  4.8941e-02,  1.9541e-02,\n",
      "        -5.5252e-03,  3.4766e-02, -1.3416e-02,  7.0290e-02, -9.7227e-03,\n",
      "        -5.2023e-02, -4.5597e-02, -2.9911e-03,  2.2905e-02, -4.5266e-02,\n",
      "        -3.8293e-02,  1.9257e-02,  6.1705e-02, -7.2137e-03, -6.4386e-03,\n",
      "        -5.8993e-02, -6.3635e-02, -5.1172e-02, -2.1474e-02,  5.6160e-04,\n",
      "         7.3992e-03,  9.7293e-03, -1.8194e-02,  5.8338e-03,  3.7840e-02,\n",
      "        -1.0026e-02,  2.2472e-02, -2.9799e-03,  3.0015e-02,  2.1913e-02,\n",
      "        -1.6694e-02,  1.4551e-02,  2.2794e-02,  1.2725e-02, -1.2813e-02,\n",
      "         1.4224e-02, -1.9654e-02,  5.1163e-02, -6.6237e-02, -2.2005e-02,\n",
      "        -3.4572e-02,  1.2666e-03,  3.6816e-02, -3.3796e-03, -6.0114e-03,\n",
      "        -2.6951e-02,  2.8808e-02, -1.4202e-02,  1.6017e-02,  1.0514e-02,\n",
      "         4.5350e-02,  2.5642e-02,  2.4090e-02,  4.7057e-02, -1.0526e-02,\n",
      "        -8.7865e-03,  3.8751e-03, -1.4291e-02, -1.2814e-02,  6.4241e-02,\n",
      "        -4.6862e-02, -1.8674e-02,  1.0244e-02, -2.5539e-02, -1.0171e-02,\n",
      "         2.4674e-04,  1.4987e-02, -1.1995e-02,  2.3370e-02,  1.9569e-02,\n",
      "        -2.6636e-02,  5.1960e-03,  2.3475e-02,  2.1167e-02,  2.8923e-03,\n",
      "        -1.1644e-02,  1.1655e-03, -9.1523e-03,  1.9265e-02, -1.4003e-02,\n",
      "        -1.0786e-02, -2.1334e-02, -1.1801e-02,  2.2192e-02, -3.7840e-02,\n",
      "        -5.8401e-02, -3.2528e-03,  1.0508e-03, -2.3902e-02, -1.1851e-04,\n",
      "         2.6648e-02,  1.0436e-02, -6.0071e-02, -1.0809e-02, -3.8385e-02,\n",
      "         2.7039e-03, -2.2861e-02, -2.3970e-03, -2.9505e-02, -1.9569e-02,\n",
      "        -1.5112e-02, -1.1818e-02, -2.3214e-02, -4.4718e-02, -2.0141e-02,\n",
      "         2.0536e-02, -3.0825e-02, -2.9510e-02, -1.6246e-02,  4.2064e-02,\n",
      "         1.1527e-02, -6.3050e-02,  2.2743e-02,  4.2392e-02,  3.8362e-02,\n",
      "        -2.9204e-02, -1.3321e-03,  4.5630e-02,  3.2790e-02,  8.6312e-03,\n",
      "         3.9444e-03, -1.6840e-02,  2.0393e-02,  2.5829e-02, -2.6723e-02,\n",
      "        -2.2201e-02,  1.3232e-02, -6.7439e-02, -2.2364e-02,  5.6992e-03,\n",
      "         1.2293e-02, -7.1308e-03, -2.6152e-02, -4.3574e-03, -3.1458e-02,\n",
      "         5.3002e-02, -1.0658e-02,  4.9902e-02,  7.0980e-03,  2.3503e-02,\n",
      "        -2.9209e-02,  2.2887e-02, -4.3271e-02, -1.0546e-02, -8.5992e-04,\n",
      "        -1.0860e-03, -1.4332e-03,  2.2155e-02,  1.3470e-02,  2.9084e-02,\n",
      "        -2.6335e-02, -2.5792e-02, -6.2941e-02, -5.4506e-02,  2.7839e-02,\n",
      "         5.9264e-02, -4.6552e-03, -3.6333e-02, -2.5720e-02, -3.9555e-02,\n",
      "         3.5746e-02, -5.4661e-02, -7.2238e-03, -2.3533e-02,  5.1847e-03,\n",
      "         6.5663e-02,  2.4837e-03, -3.4101e-02, -1.6710e-02, -4.7090e-03,\n",
      "        -3.5251e-03, -2.6543e-02, -2.4564e-02, -3.9155e-02, -2.1740e-02,\n",
      "         1.9267e-02,  5.7205e-02, -1.6727e-02, -7.2846e-03,  2.7331e-02,\n",
      "        -1.9527e-02,  1.1877e-02,  9.4022e-03, -1.5823e-02, -8.0902e-03,\n",
      "        -1.3314e-03, -4.2347e-02, -2.2915e-02,  5.9318e-02, -1.3526e-02,\n",
      "         5.5669e-02,  2.2128e-02,  4.3442e-02,  4.8556e-02, -3.1538e-03,\n",
      "         2.3287e-02,  7.6887e-03, -3.8528e-02,  3.0603e-02, -1.3844e-03,\n",
      "         2.7112e-02,  1.1429e-02, -3.4682e-02, -1.5918e-02, -3.5867e-02,\n",
      "         3.5861e-02, -1.2086e-02, -2.0581e-02,  2.3238e-02,  5.4454e-02,\n",
      "         2.2233e-02,  6.6475e-02, -2.9853e-02, -5.5693e-03,  4.0315e-02,\n",
      "         1.7253e-02, -4.3912e-02, -2.3705e-02, -3.3213e-02,  3.8856e-03,\n",
      "         1.2916e-02, -4.4598e-04, -6.6086e-02,  3.6583e-03, -2.6494e-03,\n",
      "         7.7681e-03,  1.9576e-02,  3.1406e-02, -4.4384e-02,  1.6857e-02,\n",
      "        -2.8796e-02,  3.4063e-02,  2.4577e-02,  2.0501e-02,  2.2328e-02,\n",
      "         2.7152e-02, -7.2320e-04, -4.6486e-02,  1.0273e-02, -9.6207e-03,\n",
      "        -4.2170e-02, -3.0856e-02,  6.1294e-02, -4.1007e-02, -4.5154e-02,\n",
      "        -3.0413e-02, -3.4624e-02, -7.5980e-03,  1.0514e-03, -3.3193e-02,\n",
      "         1.2548e-02,  4.8595e-03, -9.5899e-05, -1.7307e-02,  3.1489e-02,\n",
      "        -4.6316e-02, -2.0256e-02, -3.1465e-02, -2.3768e-02, -3.2722e-02,\n",
      "        -3.2816e-02, -1.5161e-02,  3.6628e-02, -5.0692e-03, -1.2813e-03,\n",
      "        -3.2450e-04, -9.9619e-04,  2.4994e-02, -3.5756e-02,  5.9012e-02,\n",
      "        -3.9452e-03,  2.4360e-03,  3.7260e-02,  9.5665e-03,  3.8291e-02,\n",
      "        -1.5048e-02,  6.1349e-02, -4.0649e-02,  7.2849e-03,  1.9998e-02,\n",
      "         1.8387e-02, -3.2916e-02,  4.3233e-02, -3.1634e-02, -2.3056e-02,\n",
      "         4.8005e-02, -1.7906e-02,  1.9458e-02, -1.9784e-03,  4.7338e-02,\n",
      "         1.4753e-02, -3.2467e-02, -6.8417e-02,  4.3973e-02,  1.6828e-02,\n",
      "        -2.1364e-02, -2.4663e-02, -5.4697e-02, -1.5644e-02,  8.6422e-03,\n",
      "        -4.4072e-02, -7.2345e-03, -4.2374e-02,  3.1124e-02, -2.2392e-02,\n",
      "        -5.1708e-02,  1.6299e-02,  2.9046e-02,  1.4808e-02,  1.7983e-02,\n",
      "         1.2550e-02, -3.8565e-02,  4.9358e-02, -1.4730e-02,  2.5914e-02,\n",
      "         5.1934e-03, -5.0209e-02, -5.0155e-03,  5.1062e-02, -1.6150e-02,\n",
      "         1.4531e-03,  6.1821e-03,  5.4210e-02, -3.0316e-02, -1.3523e-02,\n",
      "         1.9947e-02, -7.9925e-03,  2.6262e-02, -5.0759e-02,  4.1231e-02,\n",
      "        -1.0973e-02, -1.7466e-02, -3.2100e-02, -1.3387e-03,  3.3065e-02,\n",
      "        -7.6808e-04,  2.1425e-02, -3.2060e-02, -2.5195e-02, -4.2522e-02,\n",
      "        -1.3927e-02,  8.6624e-05, -1.3842e-02,  8.8855e-03,  3.8045e-02,\n",
      "         3.8306e-02, -2.4866e-03,  3.4300e-02, -4.2430e-02,  1.5327e-03,\n",
      "         3.3564e-02,  2.6359e-02,  5.3408e-02, -2.1682e-02,  3.8900e-02,\n",
      "         5.5135e-02,  1.5800e-02, -1.9902e-03,  8.2426e-06, -7.8338e-04,\n",
      "         5.1989e-02,  2.5964e-02, -1.2189e-02, -1.8365e-02,  7.5271e-03,\n",
      "        -3.9939e-02,  3.3704e-02,  2.7504e-02, -2.1811e-02, -5.1286e-03,\n",
      "         2.8058e-02, -7.7349e-03,  1.2280e-02,  5.0344e-02,  6.8264e-02,\n",
      "         8.9642e-03,  4.3033e-03,  4.7700e-03, -6.6330e-02, -4.9886e-02,\n",
      "        -3.3477e-02,  3.3230e-02,  1.5827e-02, -1.2797e-02, -3.0789e-02,\n",
      "        -5.7868e-02,  5.4389e-03,  6.4398e-02, -2.3170e-02,  3.4781e-02,\n",
      "         1.6974e-02, -1.1342e-03, -2.0155e-02,  2.5220e-02, -2.4919e-02,\n",
      "         1.3042e-04,  3.1249e-03,  1.8085e-02, -1.1524e-02,  2.9426e-02,\n",
      "         5.7996e-03, -2.1135e-02,  4.7809e-02,  3.8015e-02, -4.8632e-02,\n",
      "         1.5954e-02,  1.0550e-02,  4.4322e-02, -4.6365e-02, -6.9497e-03,\n",
      "        -1.6789e-02, -4.8698e-02,  2.8175e-02,  3.3122e-02,  9.2632e-03,\n",
      "         2.9624e-02, -1.4307e-03,  4.5015e-02, -4.6675e-03, -1.1916e-02,\n",
      "        -6.9771e-02, -3.9326e-02, -4.3732e-02, -6.9605e-03,  4.6500e-02,\n",
      "        -2.1927e-02,  2.7500e-02,  2.6852e-02, -4.0941e-03,  3.9245e-02,\n",
      "         3.1266e-02,  7.1523e-02, -3.8829e-02, -7.1593e-03,  5.3229e-03,\n",
      "         3.8135e-02, -2.4916e-02,  4.8570e-03, -4.1209e-02, -4.7189e-02,\n",
      "         2.5437e-02, -3.6571e-02,  2.6438e-02, -3.0647e-02, -2.4367e-02,\n",
      "        -1.5899e-02,  4.4824e-02,  1.6655e-02,  3.1160e-02,  2.7343e-02,\n",
      "        -4.3884e-03, -3.1279e-02,  1.8112e-02, -4.1643e-02,  4.7637e-03,\n",
      "         4.0766e-02, -1.7687e-03,  3.8913e-02, -1.4729e-02,  2.4563e-02,\n",
      "        -3.8219e-02, -1.1696e-02])), ('6.weight', tensor([[-0.0043, -0.0165, -0.0277,  ..., -0.0258, -0.0354,  0.0155],\n",
      "        [ 0.0180,  0.0168,  0.0259,  ...,  0.0140, -0.0334, -0.0136],\n",
      "        [ 0.0002, -0.0226,  0.0350,  ...,  0.0316, -0.0015, -0.0047],\n",
      "        ...,\n",
      "        [ 0.0443, -0.0087,  0.0338,  ..., -0.0088,  0.0079, -0.0672],\n",
      "        [ 0.0183, -0.0113,  0.0347,  ...,  0.0212, -0.0281,  0.0089],\n",
      "        [-0.0314, -0.0074, -0.0340,  ..., -0.0404, -0.0402, -0.0257]])), ('6.bias', tensor([-0.0459,  0.0155, -0.0086, -0.0242, -0.0116,  0.0052, -0.0175, -0.0060,\n",
      "        -0.0298,  0.0259, -0.0280, -0.0019,  0.0257, -0.0485,  0.0378, -0.0385,\n",
      "         0.0358,  0.0338,  0.0725,  0.0230, -0.0205, -0.0252,  0.0504,  0.0503,\n",
      "        -0.0290, -0.0201, -0.0024, -0.0441,  0.0623, -0.0089,  0.0261, -0.0357,\n",
      "        -0.0054,  0.0289,  0.0176,  0.0220,  0.0656, -0.0022,  0.0155, -0.0076,\n",
      "         0.0158,  0.0284, -0.0143, -0.0077, -0.0381, -0.0520,  0.0023, -0.0043,\n",
      "        -0.0310, -0.0436,  0.0013,  0.0195,  0.0065, -0.0388,  0.0712,  0.0049,\n",
      "         0.0385, -0.0010,  0.0478,  0.0033, -0.0153,  0.0555,  0.0241, -0.0228,\n",
      "        -0.0481,  0.0147,  0.0149,  0.0110,  0.0072, -0.0011, -0.0491, -0.0051,\n",
      "         0.0411,  0.0446,  0.0189, -0.0121, -0.0133,  0.0559, -0.0386,  0.0342,\n",
      "         0.0528, -0.0382, -0.0002,  0.0142, -0.0222, -0.0541,  0.0689,  0.0146,\n",
      "        -0.0167, -0.0361,  0.0006,  0.0637,  0.0441, -0.0522,  0.0467, -0.0145,\n",
      "         0.0388, -0.0308, -0.0524,  0.0345,  0.0611,  0.0417, -0.0205,  0.0300,\n",
      "         0.0119,  0.0299,  0.0006,  0.0178,  0.0067,  0.0111,  0.0237,  0.0019,\n",
      "         0.0593,  0.0500,  0.0260,  0.0206,  0.0617,  0.0585,  0.0199, -0.0429,\n",
      "         0.0397, -0.0333, -0.0443, -0.0019, -0.0191, -0.0387, -0.0429, -0.0278,\n",
      "         0.0418, -0.0331, -0.0430,  0.0117,  0.0031,  0.0330,  0.0081,  0.0118,\n",
      "         0.0352,  0.0116,  0.0178, -0.0254,  0.0712, -0.0362, -0.0134, -0.0762,\n",
      "         0.0347, -0.0411,  0.0738,  0.0497,  0.0530, -0.0043, -0.0127,  0.0235,\n",
      "        -0.0431, -0.0438,  0.0052, -0.0138,  0.0435, -0.0279,  0.0254, -0.0040,\n",
      "        -0.0032,  0.0551, -0.0426, -0.0115, -0.0102, -0.0077, -0.0208, -0.0122,\n",
      "         0.0746, -0.0142,  0.0452,  0.0248,  0.0090,  0.0243, -0.0095, -0.0123,\n",
      "        -0.0317,  0.0197, -0.0013,  0.0130, -0.0184, -0.0010,  0.0156,  0.0582,\n",
      "        -0.0396,  0.0660,  0.0763, -0.0517,  0.0593, -0.0013, -0.0285, -0.0093,\n",
      "         0.0034, -0.0432,  0.0260,  0.0223, -0.0145, -0.0050,  0.0156, -0.0208,\n",
      "         0.0064,  0.0485,  0.0176, -0.0040, -0.0022, -0.0169, -0.0076,  0.0557,\n",
      "         0.0546,  0.0064,  0.0144,  0.0080,  0.0214, -0.0220,  0.0345,  0.0381,\n",
      "         0.0210, -0.0460,  0.0221,  0.0263, -0.0090,  0.0269,  0.0003,  0.0531,\n",
      "        -0.0027, -0.0082, -0.0260, -0.0385, -0.0483, -0.0305,  0.0075,  0.0536,\n",
      "         0.0137,  0.0732, -0.0063, -0.0299, -0.0086,  0.0516, -0.0488, -0.0410,\n",
      "        -0.0048,  0.0763, -0.0082, -0.0115, -0.0108, -0.0217,  0.0505,  0.0443,\n",
      "         0.0592, -0.0614, -0.0327, -0.0268,  0.0324, -0.0431,  0.0261, -0.0055,\n",
      "         0.0139, -0.0003, -0.0043, -0.0321,  0.0547,  0.0272, -0.0056, -0.0235,\n",
      "         0.0365, -0.0046,  0.0300,  0.0246,  0.0488,  0.0043,  0.0478,  0.0279,\n",
      "         0.0106, -0.0028,  0.0443, -0.0035,  0.0700,  0.0180, -0.0406, -0.0277,\n",
      "        -0.0486, -0.0312, -0.0125,  0.0193,  0.0512, -0.0376,  0.0296, -0.0248,\n",
      "        -0.0147,  0.0676,  0.0164,  0.0081,  0.0132,  0.0083,  0.0390,  0.0377,\n",
      "         0.0356,  0.0038, -0.0346,  0.0257,  0.0754,  0.0084,  0.0251, -0.0110,\n",
      "        -0.0202, -0.0460,  0.0207,  0.0106,  0.0073,  0.0690,  0.0392,  0.0218,\n",
      "         0.0406, -0.0310, -0.0079, -0.0393, -0.0025,  0.0199, -0.0063,  0.0665,\n",
      "        -0.0012,  0.0214, -0.0249, -0.0523,  0.0229, -0.0238,  0.0025, -0.0313,\n",
      "         0.0486, -0.0384, -0.0150,  0.0509, -0.0122, -0.0190,  0.0265, -0.0021,\n",
      "        -0.0438, -0.0056,  0.0440, -0.0459,  0.0136,  0.0558, -0.0461, -0.0104,\n",
      "        -0.0461,  0.0037, -0.0023,  0.0142,  0.0512, -0.0213,  0.0047, -0.0334,\n",
      "        -0.0395,  0.0325, -0.0370,  0.0497,  0.0049,  0.0051,  0.0653,  0.0050,\n",
      "        -0.0020,  0.0147,  0.0356,  0.0256, -0.0487, -0.0458,  0.0587, -0.0547,\n",
      "        -0.0117,  0.0659, -0.0474,  0.0198, -0.0190,  0.0173, -0.0436, -0.0357,\n",
      "        -0.0304, -0.0157, -0.0253,  0.0113,  0.0310, -0.0212,  0.0136, -0.0032,\n",
      "        -0.0136, -0.0142,  0.0275, -0.0044, -0.0412, -0.0069, -0.0090,  0.0382,\n",
      "        -0.0233, -0.0096,  0.0219,  0.0004, -0.0153, -0.0189,  0.0114,  0.0162,\n",
      "        -0.0431, -0.0198, -0.0387,  0.0582,  0.0724,  0.0125, -0.0110, -0.0281,\n",
      "        -0.0254, -0.0204, -0.0035, -0.0358,  0.0065, -0.0064,  0.0213,  0.0614,\n",
      "         0.0233,  0.0246, -0.0235, -0.0446, -0.0040, -0.0316,  0.0694, -0.0060,\n",
      "        -0.0201,  0.0204, -0.0434, -0.0069, -0.0078, -0.0376, -0.0295, -0.0216,\n",
      "        -0.0345,  0.0323,  0.0091, -0.0020, -0.0308, -0.0619,  0.0239,  0.0310,\n",
      "        -0.0143, -0.0487, -0.0254,  0.0206,  0.0706,  0.0379, -0.0229, -0.0239,\n",
      "         0.0116, -0.0188, -0.0158,  0.0094,  0.0093, -0.0171,  0.0246,  0.0510,\n",
      "         0.0084,  0.0569,  0.0586, -0.0420, -0.0166,  0.0521,  0.0092,  0.0269,\n",
      "        -0.0120,  0.0105,  0.0027, -0.0147, -0.0146,  0.0090,  0.0598, -0.0391,\n",
      "         0.0327, -0.0238, -0.0220, -0.0272, -0.0447,  0.0481, -0.0072,  0.0196,\n",
      "         0.0472,  0.0379, -0.0317,  0.0235, -0.0494,  0.0205,  0.0142, -0.0200,\n",
      "         0.0017, -0.0256, -0.0040, -0.0013,  0.0443,  0.0299,  0.0248,  0.0486,\n",
      "        -0.0154,  0.0782,  0.0543,  0.0060, -0.0310, -0.0233, -0.0014,  0.0167,\n",
      "        -0.0308, -0.0088,  0.0329,  0.0451,  0.0328,  0.0513,  0.0386, -0.0291])), ('8.weight', tensor([[-0.0522,  0.0132, -0.0085,  ...,  0.0222,  0.0304,  0.0268],\n",
      "        [-0.0142, -0.0050,  0.0037,  ..., -0.0092, -0.0025, -0.0128],\n",
      "        [ 0.0201,  0.0324,  0.0271,  ...,  0.0200,  0.0272,  0.0315],\n",
      "        ...,\n",
      "        [-0.0113,  0.0099,  0.0325,  ...,  0.0189,  0.0325,  0.0091],\n",
      "        [ 0.0230,  0.0294, -0.0518,  ..., -0.0371, -0.0095, -0.0092],\n",
      "        [ 0.0307, -0.0291, -0.0325,  ...,  0.0195,  0.0335,  0.0280]])), ('8.bias', tensor([-0.0219, -0.0273,  0.0316, -0.0489, -0.0249,  0.0595,  0.0079,  0.0591,\n",
      "        -0.0350,  0.0422, -0.0135, -0.0113,  0.0167, -0.0321, -0.0159, -0.0345,\n",
      "         0.0533, -0.0091,  0.0448,  0.0188,  0.0211, -0.0657,  0.0502,  0.0123,\n",
      "        -0.0050,  0.0555, -0.0154,  0.0564,  0.0676, -0.0572,  0.0713,  0.0521,\n",
      "         0.0601,  0.0403, -0.0099, -0.0251, -0.0337,  0.0297,  0.0199, -0.0539,\n",
      "        -0.0367, -0.0517,  0.0318,  0.0039,  0.0497, -0.0094, -0.0469,  0.0657,\n",
      "         0.0131, -0.0309,  0.0298,  0.0174, -0.0106,  0.0502, -0.0090, -0.0074,\n",
      "         0.0613, -0.0394,  0.0590, -0.0375,  0.0467,  0.0124,  0.0562, -0.0477,\n",
      "        -0.0221,  0.0565, -0.0240,  0.0402,  0.0384, -0.0197, -0.0168, -0.0536,\n",
      "         0.0505,  0.0696, -0.0350,  0.1000,  0.0625,  0.0328, -0.0013,  0.0064,\n",
      "         0.0267, -0.0172,  0.0041, -0.0334, -0.0174,  0.0631, -0.0209,  0.0768,\n",
      "         0.0059,  0.0545, -0.0340,  0.0395,  0.0754,  0.0239,  0.0151,  0.0062,\n",
      "        -0.0348,  0.0186, -0.0127,  0.0358, -0.0333, -0.0476,  0.0592, -0.0449,\n",
      "        -0.0045,  0.0117, -0.0274,  0.0381,  0.0738, -0.0263,  0.0593, -0.0236,\n",
      "        -0.0048,  0.0758, -0.0267, -0.0912,  0.0512, -0.0534, -0.0200,  0.0384,\n",
      "         0.0764, -0.0394,  0.0712,  0.0087, -0.0460,  0.0582, -0.0004,  0.0041,\n",
      "        -0.0357,  0.0602,  0.0227, -0.0418,  0.0327, -0.0021,  0.0527, -0.0366,\n",
      "        -0.0165, -0.0099,  0.0328,  0.0486,  0.0043, -0.0212, -0.0275,  0.0681,\n",
      "        -0.0437, -0.0840,  0.0269, -0.0091, -0.0091,  0.0257, -0.0449, -0.0017,\n",
      "         0.0370, -0.0260, -0.0093,  0.0820, -0.0443,  0.0598,  0.0269,  0.0389,\n",
      "         0.0513,  0.0671,  0.0631,  0.0257,  0.0633, -0.0475,  0.0023, -0.0587,\n",
      "        -0.0416,  0.0490,  0.0010,  0.0318, -0.0492, -0.0197, -0.0156, -0.0021,\n",
      "         0.0254, -0.0716,  0.0402,  0.0454,  0.0275,  0.0040,  0.0629,  0.0430,\n",
      "         0.0616,  0.0601,  0.0174, -0.0283,  0.0602,  0.0420, -0.0499, -0.0797,\n",
      "         0.0404, -0.0305,  0.0445, -0.0338,  0.0443, -0.0278,  0.0323,  0.0061,\n",
      "        -0.0409, -0.0367, -0.0456,  0.0577,  0.0395, -0.0436, -0.0584, -0.0185,\n",
      "        -0.0277,  0.0377,  0.0572,  0.0318,  0.0815,  0.0443,  0.0360,  0.0530,\n",
      "         0.0577,  0.0616, -0.0262,  0.0748, -0.0319,  0.0314,  0.0408, -0.0218,\n",
      "        -0.0454, -0.0496,  0.0167,  0.0383, -0.0445,  0.0147,  0.0753,  0.0564,\n",
      "         0.0353,  0.0476,  0.0277,  0.0540, -0.0099, -0.0219,  0.0548, -0.0291,\n",
      "        -0.0438, -0.0031,  0.0533,  0.0025, -0.0139,  0.0515, -0.0061,  0.0146,\n",
      "        -0.0372, -0.0073,  0.0350, -0.0424, -0.0408, -0.0018,  0.0423, -0.0350])), ('10.weight', tensor([[ 2.3065e-02,  1.0013e-02,  3.3038e-02,  3.5222e-02, -1.1435e-03,\n",
      "          2.7408e-02, -5.3475e-02, -1.7187e-02,  2.2148e-02,  1.6905e-02,\n",
      "          3.4899e-02, -2.4175e-02,  3.1475e-02,  2.9951e-02,  4.4982e-02,\n",
      "         -1.7391e-02,  3.8365e-02, -8.2691e-03,  5.4733e-02, -5.2976e-03,\n",
      "          6.1094e-02,  1.9388e-03,  4.7392e-02,  2.3063e-02, -2.1566e-02,\n",
      "         -3.9830e-02,  3.6964e-02, -3.7866e-02, -3.0928e-02,  3.0892e-02,\n",
      "          5.7511e-02, -4.8332e-02, -2.8889e-02,  3.1107e-02,  1.4457e-03,\n",
      "         -4.1978e-02,  1.5566e-02, -4.8746e-02,  1.5693e-03,  4.6566e-02,\n",
      "         -1.8366e-03,  9.8666e-03, -3.6832e-02, -3.6374e-02,  4.8331e-02,\n",
      "          2.2058e-02,  1.5952e-02, -4.8812e-02, -1.7531e-03, -3.7992e-02,\n",
      "          4.3378e-02,  2.6073e-03,  3.7412e-02,  4.5431e-02,  3.9155e-02,\n",
      "         -8.0447e-03, -5.7614e-02,  4.4596e-03,  7.1105e-02, -7.7052e-03,\n",
      "         -3.6647e-02,  1.2188e-03,  6.4835e-02, -2.5564e-02, -9.3703e-03,\n",
      "          4.0955e-02,  1.1230e-02,  3.8418e-02,  6.4349e-02, -2.3479e-02,\n",
      "         -1.7113e-03,  1.8442e-02,  6.4190e-02,  6.6620e-02, -1.8688e-02,\n",
      "         -5.0372e-02, -6.2659e-02,  1.0091e-03,  3.6559e-02,  9.1947e-03,\n",
      "         -4.2306e-02,  5.6146e-05, -8.2130e-03, -2.8219e-03, -3.2454e-02,\n",
      "          4.6939e-02, -3.3253e-02, -7.4771e-02, -3.7815e-03, -3.6111e-02,\n",
      "          3.4435e-02,  1.4338e-02, -6.2075e-02, -3.1924e-02,  2.0251e-04,\n",
      "         -3.1376e-02, -4.1661e-02,  2.6492e-02,  1.1545e-03, -3.9894e-03,\n",
      "         -4.0783e-02, -2.3076e-02, -1.5819e-02,  1.2782e-02,  4.0523e-02,\n",
      "          6.6902e-02, -5.1750e-02,  2.7210e-02, -2.6858e-02,  1.4686e-03,\n",
      "          6.0607e-03, -5.2741e-03,  8.7589e-03, -5.3028e-02, -3.1305e-02,\n",
      "          5.3783e-02,  2.9594e-02, -8.6878e-04,  3.7768e-02, -1.1499e-02,\n",
      "         -5.0909e-02,  3.5773e-02, -4.6365e-02, -2.2281e-02, -3.6226e-02,\n",
      "          2.0099e-02, -2.1793e-02, -1.5593e-02,  1.6558e-02, -1.2189e-02,\n",
      "         -2.0346e-02, -2.6291e-03,  2.4309e-02, -4.2361e-02,  2.1762e-02,\n",
      "          3.2094e-02, -4.0409e-02, -9.7673e-03, -5.1835e-02,  3.1118e-02,\n",
      "          5.0408e-02,  6.5300e-03, -8.4449e-03,  2.4611e-02, -1.7461e-02,\n",
      "          4.6021e-03,  4.0661e-02, -2.2019e-02,  5.5983e-02, -1.8071e-03,\n",
      "         -8.5779e-04,  2.6984e-03, -4.1028e-03, -3.8794e-03,  1.0130e-02,\n",
      "         -3.1871e-02, -3.6964e-02,  6.2676e-02, -5.7474e-02, -4.8608e-02,\n",
      "         -2.2193e-02, -4.5877e-02, -4.0228e-02,  5.0661e-02,  5.8251e-02,\n",
      "         -8.6807e-03,  8.6130e-03, -1.9779e-02,  2.7930e-02, -7.1535e-03,\n",
      "          1.4421e-02,  4.2639e-04, -5.5635e-02, -5.6671e-05,  1.4053e-02,\n",
      "         -4.4225e-02, -5.6899e-02, -1.7253e-02,  2.9520e-02,  1.0602e-02,\n",
      "          4.1687e-02,  2.8916e-03, -4.1898e-02, -5.0848e-02, -2.5352e-02,\n",
      "          2.4558e-03,  2.3991e-02, -1.0620e-02,  2.8827e-02, -4.0518e-02,\n",
      "          1.0202e-02, -9.8175e-03, -2.9410e-02,  5.7951e-03,  2.0267e-03,\n",
      "          9.7571e-03, -3.8564e-02, -3.8072e-02, -2.8232e-03,  5.7876e-02,\n",
      "          5.0993e-02,  4.2474e-03,  3.0646e-02,  5.4461e-02, -3.7019e-02,\n",
      "          1.4945e-02,  4.7844e-03,  5.4822e-02, -3.4688e-02,  5.1667e-02,\n",
      "          6.5300e-02, -1.1384e-02, -3.7441e-02,  1.0014e-02,  2.7326e-02,\n",
      "         -2.2953e-02,  5.5142e-02, -3.4000e-02, -2.1446e-02, -4.4298e-02,\n",
      "          1.3640e-02,  2.9944e-02, -4.4215e-02, -9.1365e-03, -1.2548e-02,\n",
      "          8.7004e-04, -6.5802e-03, -5.0097e-02,  2.1786e-02,  5.1207e-03,\n",
      "         -7.1678e-02,  2.7976e-02,  4.4690e-04,  2.8718e-02,  3.9001e-03,\n",
      "          1.8025e-02,  1.5575e-02, -1.3993e-02,  2.4768e-02,  4.6163e-02,\n",
      "         -5.0149e-02, -2.8603e-04,  4.9664e-02, -3.2332e-02,  1.2546e-02,\n",
      "          1.0987e-02,  4.8014e-02,  9.1197e-03,  4.4424e-03, -4.2698e-02,\n",
      "          1.2892e-02,  2.2221e-02, -2.2495e-02,  5.5536e-03,  5.6490e-03,\n",
      "          5.2353e-02]])), ('10.bias', tensor([-0.0506]))])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_neunet = NeuNet(model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_neunet.load_checkpoint(\"model_checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('0.weight', tensor([[ 0.0975,  0.0945, -0.0405,  ...,  0.0604,  0.0483,  0.0719],\n",
      "        [ 0.0346,  0.0134, -0.0038,  ..., -0.0303, -0.0512, -0.0325],\n",
      "        [ 0.0499,  0.0428,  0.0457,  ...,  0.0619,  0.0425, -0.0277],\n",
      "        ...,\n",
      "        [ 0.0344, -0.0371, -0.0556,  ..., -0.0621,  0.0601, -0.0254],\n",
      "        [ 0.0227,  0.0221, -0.0570,  ...,  0.0116,  0.0552,  0.0548],\n",
      "        [ 0.0075, -0.0497, -0.0140,  ...,  0.0278, -0.0025, -0.0101]])), ('0.bias', tensor([-4.8642e-02, -6.4022e-02,  2.8082e-02, -2.4446e-02,  4.6268e-02,\n",
      "         4.8874e-02,  4.0562e-02,  1.4077e-02, -6.5047e-03, -2.7288e-02,\n",
      "        -3.2381e-02, -5.2258e-02,  5.7791e-02, -1.4139e-02, -2.0281e-02,\n",
      "        -6.7407e-02, -1.7636e-02, -2.2741e-02,  5.3100e-02, -2.0805e-03,\n",
      "         6.1342e-02,  6.9020e-03, -5.3025e-02, -5.0474e-02,  8.7506e-03,\n",
      "        -2.3386e-02, -9.9139e-03, -5.6342e-03, -2.0960e-02,  5.3774e-02,\n",
      "        -3.1210e-02,  2.2367e-02, -5.9626e-02, -3.1538e-05, -2.9136e-02,\n",
      "         5.8115e-03,  2.4725e-02, -3.7035e-02, -5.6828e-03,  5.2912e-04,\n",
      "         5.4402e-02, -5.6243e-02, -4.1138e-03, -6.9213e-02,  2.5951e-02,\n",
      "        -1.7000e-02,  2.1199e-02, -2.5296e-02,  5.3274e-02, -5.0056e-02,\n",
      "         2.5693e-03,  4.6974e-02, -5.0460e-03, -1.7683e-02, -6.0072e-02,\n",
      "        -4.2820e-02, -1.0650e-02, -6.2043e-02,  5.1721e-02, -1.8055e-02,\n",
      "         3.8527e-02,  4.4973e-02, -1.0230e-02, -2.2303e-02, -3.0911e-02,\n",
      "        -7.3005e-02,  1.6897e-02, -4.8383e-02,  3.7639e-02, -2.0988e-02,\n",
      "        -5.3435e-02,  4.7270e-02,  5.3636e-02, -5.4171e-02,  4.4271e-02,\n",
      "        -4.3512e-02,  3.2336e-03, -5.4153e-02, -6.8034e-02, -2.5893e-02,\n",
      "         5.1237e-02, -4.1103e-02, -6.4710e-02,  3.5298e-02, -2.2614e-02,\n",
      "        -4.6217e-02, -1.0791e-02, -3.3625e-02, -5.9675e-02,  1.5499e-02,\n",
      "         2.0383e-02, -3.2027e-03,  3.6331e-02,  3.6816e-03, -6.3648e-02,\n",
      "         1.4258e-02, -4.8522e-02, -2.7051e-02, -3.6488e-02, -3.8826e-02,\n",
      "        -5.1397e-03,  2.5937e-02,  4.9849e-02, -5.5359e-02,  1.8927e-02,\n",
      "        -1.7664e-02, -1.2980e-02,  7.8198e-03,  4.5043e-02, -2.5396e-02,\n",
      "         5.4157e-02,  8.6602e-03, -1.4607e-02,  5.2225e-02, -5.6123e-02,\n",
      "        -4.1338e-02, -2.0248e-02,  3.2728e-02, -2.2108e-02, -5.0172e-02,\n",
      "         3.4735e-02, -5.8321e-02,  7.1731e-03, -4.8331e-02, -7.7569e-02,\n",
      "        -3.7444e-02,  2.2990e-02,  3.8560e-02,  5.4977e-02, -8.3112e-03,\n",
      "        -3.2454e-02, -4.0370e-02, -2.8887e-02, -5.6852e-02,  7.3587e-02,\n",
      "         2.4312e-02, -6.8205e-02, -4.5055e-02,  7.9125e-03,  4.6306e-02,\n",
      "         5.1615e-02, -4.2924e-02, -1.2688e-02,  2.8116e-02, -1.7106e-02,\n",
      "        -6.1883e-02,  5.4930e-02,  5.6030e-02, -5.7942e-02, -9.4925e-03,\n",
      "        -1.4237e-02, -4.3242e-02, -8.6611e-04, -5.6515e-02, -1.1549e-02,\n",
      "         2.9527e-02,  2.3212e-02,  9.0658e-03, -6.1306e-02, -3.4416e-02,\n",
      "        -5.4803e-02, -2.7555e-02, -2.3886e-02, -5.2712e-02, -4.8245e-02,\n",
      "        -3.1079e-02,  6.0649e-02, -5.0808e-03, -5.5965e-02, -3.4043e-02,\n",
      "         6.2548e-02,  3.5105e-02,  2.7381e-02,  1.0989e-02, -2.3912e-03,\n",
      "         9.7919e-03, -7.0966e-02, -2.3756e-02, -3.1724e-02,  4.0793e-02,\n",
      "        -1.9529e-02, -2.6195e-02, -5.8935e-02, -5.7593e-02,  8.1745e-03,\n",
      "         3.8979e-02,  5.8963e-02, -2.2004e-02, -3.7185e-02, -4.4911e-02,\n",
      "         4.6683e-02,  3.7164e-02,  4.7750e-02,  1.7558e-03, -1.8898e-03,\n",
      "         3.6047e-02, -1.3830e-03, -3.0255e-02, -6.1283e-02,  3.8210e-02,\n",
      "         2.6766e-02, -1.1344e-02, -2.1600e-02, -1.7184e-02, -3.2148e-02,\n",
      "         7.7460e-03,  9.6666e-04,  4.7235e-02,  6.3284e-02,  2.0698e-02,\n",
      "        -4.6466e-02, -4.2151e-02,  3.8905e-02, -9.7885e-03, -7.9513e-02,\n",
      "        -4.8735e-02, -6.9717e-02, -1.5542e-03,  2.9656e-02, -4.0109e-02,\n",
      "        -3.4903e-02,  5.3734e-02,  3.6218e-02,  3.2180e-02, -6.8573e-02,\n",
      "         3.4691e-02,  4.3995e-02,  3.5278e-02,  2.6283e-02, -3.7704e-02,\n",
      "        -5.4765e-02,  3.2272e-02,  1.4203e-02,  3.5804e-02, -5.2387e-02,\n",
      "         2.7679e-03,  3.9757e-02,  1.2732e-02, -5.9701e-02, -8.4522e-02,\n",
      "         3.2481e-02, -3.0609e-02, -3.7592e-02, -6.4778e-02, -5.8358e-02,\n",
      "         4.2260e-02,  1.7855e-03,  1.2758e-02,  3.3885e-02,  3.8177e-02,\n",
      "         4.5011e-02, -3.1456e-02, -2.0097e-02,  3.0499e-03, -5.6424e-02,\n",
      "         4.1727e-02,  5.3781e-02, -3.9272e-02, -3.5936e-03, -9.4085e-04,\n",
      "         1.9811e-02,  2.5823e-02, -2.7992e-02,  2.8566e-02, -3.6656e-02,\n",
      "         5.8455e-02, -4.3516e-02,  5.4371e-02,  2.0111e-03, -3.6524e-02,\n",
      "         1.1447e-03, -2.5130e-03,  5.5424e-02,  1.5243e-02, -1.4647e-02,\n",
      "        -6.2558e-02,  5.2878e-02, -1.7253e-02,  6.6544e-02,  6.3773e-02,\n",
      "         3.4152e-02,  1.7312e-02,  2.0517e-03, -6.0314e-03,  2.0282e-02,\n",
      "        -3.8004e-02,  2.6254e-02,  5.0684e-02, -2.8352e-02, -7.3154e-02,\n",
      "         1.4783e-03, -2.5789e-02, -5.3022e-02, -5.1734e-02,  6.4064e-02,\n",
      "        -1.7080e-03, -1.9950e-02,  5.5278e-02,  4.3397e-03, -2.7617e-02,\n",
      "        -1.5173e-02,  3.3073e-02, -2.3875e-02, -5.4338e-02,  5.4431e-02,\n",
      "         7.9233e-03,  5.6654e-03,  7.8059e-03, -4.6121e-02, -2.5353e-02,\n",
      "         4.9661e-02, -4.1910e-02, -9.7961e-03, -3.4516e-02, -5.0641e-02,\n",
      "        -6.7500e-02,  3.1600e-02, -6.8272e-02,  4.4390e-02, -3.2922e-02,\n",
      "        -3.9708e-02,  4.9001e-02, -4.6688e-02,  1.2427e-02,  7.6950e-03,\n",
      "        -3.1724e-02, -6.6560e-02,  1.2663e-02, -4.6840e-02,  1.8247e-02,\n",
      "        -5.3308e-02,  5.6202e-02, -5.0806e-02, -1.1906e-02,  7.9345e-03,\n",
      "        -5.6379e-02, -2.1402e-02,  9.1360e-03,  4.2840e-02,  6.5405e-02,\n",
      "        -1.0616e-02,  5.2906e-04,  2.3657e-06,  3.4616e-02,  6.8193e-02,\n",
      "        -3.2373e-02,  7.7927e-03, -4.7102e-02,  5.1846e-02,  2.1721e-02,\n",
      "        -6.4150e-04, -2.1325e-02, -4.9479e-02, -1.5024e-03, -1.3562e-02,\n",
      "         1.7586e-02, -7.0483e-03, -1.6341e-02, -3.5230e-03, -7.1340e-02,\n",
      "        -2.5567e-02, -4.4780e-02, -1.5440e-02, -5.9330e-02, -7.3357e-02,\n",
      "        -2.1088e-02,  2.5439e-02,  3.1215e-02, -6.9068e-02, -5.8687e-02,\n",
      "        -2.6988e-02, -4.5250e-03,  5.0595e-02, -4.0059e-03, -3.5837e-02,\n",
      "        -1.2599e-02,  2.1638e-02,  3.6709e-02, -4.1495e-02, -5.4213e-02,\n",
      "        -1.9477e-02,  2.0876e-02, -3.9104e-02,  4.8362e-02, -3.4909e-02,\n",
      "         1.3263e-02, -6.7751e-02, -3.8474e-02, -1.1358e-02, -4.3594e-02,\n",
      "         1.4762e-02, -2.0709e-02,  6.5149e-02, -5.4590e-02,  4.6566e-02,\n",
      "        -9.2148e-03, -5.3685e-02,  2.3485e-02, -3.5856e-02,  4.4606e-02,\n",
      "        -2.1712e-02, -6.1369e-02, -5.3222e-02,  9.0684e-03,  4.7714e-02,\n",
      "         3.9031e-03, -5.9590e-02,  2.1937e-02,  7.4894e-03,  1.2538e-02,\n",
      "        -5.1753e-02, -1.0568e-02, -7.7530e-02,  3.5783e-02, -5.1207e-02,\n",
      "        -6.2606e-02,  5.8202e-02,  2.4376e-02,  2.5359e-02,  6.4667e-02,\n",
      "        -1.5275e-02, -4.7548e-02,  3.9731e-02, -4.2998e-02,  3.5641e-02,\n",
      "         5.7292e-02, -2.4876e-02,  1.1032e-03, -2.7467e-02, -3.0414e-02,\n",
      "        -1.7717e-02, -3.3132e-02, -2.2467e-02,  1.0378e-02, -3.4116e-02,\n",
      "         3.3469e-02,  3.3896e-02, -6.0123e-02, -3.0980e-02, -3.4814e-02,\n",
      "        -6.3164e-02,  5.3006e-02, -2.0229e-02,  5.4150e-02,  3.1225e-02,\n",
      "         3.1372e-02,  2.9221e-02, -5.6634e-02,  1.0942e-02, -2.9388e-02,\n",
      "         3.4056e-03, -1.5761e-02, -7.0149e-02,  2.8033e-02,  1.0671e-02,\n",
      "         1.1380e-02, -4.4834e-02, -5.4622e-02, -4.6182e-02, -5.7096e-02,\n",
      "        -5.7427e-02, -1.8222e-02, -5.0520e-02,  2.9502e-02, -7.1734e-02,\n",
      "        -1.0566e-02,  4.3580e-02, -4.1016e-02,  1.4907e-02,  5.1491e-02,\n",
      "        -1.0221e-02, -4.8681e-02,  5.2307e-02, -3.3413e-02,  3.4101e-02,\n",
      "         1.4638e-02, -3.8985e-02,  2.5247e-02, -4.9397e-02, -9.5636e-03,\n",
      "         1.1269e-02, -1.1933e-02,  1.2199e-02, -3.6520e-02, -8.2474e-02,\n",
      "        -1.0298e-02,  3.5593e-02, -1.9851e-02,  8.7575e-03,  1.4413e-02,\n",
      "        -1.3605e-03, -7.1743e-02, -6.2353e-02, -3.1949e-02, -3.0221e-02,\n",
      "         4.3045e-02, -6.1114e-02,  3.9586e-02,  3.9780e-02, -1.5824e-03,\n",
      "         3.7091e-02,  2.3201e-02, -6.9561e-02,  2.2336e-02,  2.1542e-02,\n",
      "         1.2292e-02, -3.4779e-02,  3.9257e-02, -8.5924e-03, -6.3732e-02,\n",
      "         1.3194e-02, -6.0701e-02])), ('2.weight', tensor([[-0.0467,  0.0154, -0.0108,  ..., -0.0116,  0.0273, -0.0093],\n",
      "        [-0.0582, -0.0367, -0.0166,  ..., -0.0260, -0.0162, -0.0534],\n",
      "        [ 0.0008, -0.0354, -0.0082,  ..., -0.0005,  0.0473,  0.0264],\n",
      "        ...,\n",
      "        [ 0.0195, -0.0436, -0.0003,  ..., -0.0055,  0.0121,  0.0023],\n",
      "        [-0.0646, -0.0194,  0.0241,  ..., -0.0558,  0.0291, -0.0187],\n",
      "        [-0.0928, -0.0342, -0.0328,  ..., -0.0465, -0.0445,  0.0338]])), ('2.bias', tensor([-3.2697e-02, -6.8235e-02, -1.3992e-02, -6.2480e-02, -8.3830e-03,\n",
      "         2.3495e-02,  1.6627e-02, -2.6087e-02, -1.1274e-02, -3.8680e-02,\n",
      "         4.5923e-02,  2.8938e-02, -1.9131e-02,  2.1139e-02,  4.7009e-02,\n",
      "        -4.4858e-02,  3.1436e-02,  8.1797e-03, -1.5408e-02, -1.3766e-03,\n",
      "        -3.5889e-02, -6.4315e-02,  5.4178e-02, -2.3289e-03, -1.9193e-02,\n",
      "        -5.7284e-02, -1.0077e-02,  4.5767e-02,  2.7289e-02, -2.0406e-02,\n",
      "        -3.5963e-03, -2.3988e-02, -3.7735e-02, -5.2427e-02,  2.0325e-02,\n",
      "        -2.8330e-02, -4.7711e-02,  1.2042e-02,  3.0137e-02, -4.3491e-02,\n",
      "        -2.7628e-02, -8.0801e-03, -6.2586e-03, -1.1027e-02, -8.0174e-03,\n",
      "        -9.2550e-03,  2.9007e-02, -1.1078e-02, -7.0354e-02,  2.0946e-02,\n",
      "         2.8106e-02,  3.7256e-03, -6.2885e-05, -3.9604e-02, -3.7868e-02,\n",
      "        -5.3404e-02,  1.1531e-02, -2.7671e-02, -3.1881e-02, -4.3119e-03,\n",
      "         1.0483e-03,  1.0395e-02,  7.1215e-03, -1.5253e-02, -2.7459e-02,\n",
      "        -6.7021e-02, -1.3497e-02, -6.0148e-02,  4.8834e-02,  1.2472e-02,\n",
      "         5.1764e-02,  4.4717e-02, -1.0764e-03, -6.1599e-02,  2.3450e-02,\n",
      "        -1.4004e-03, -6.9088e-02, -2.9103e-02,  5.2690e-03, -2.3522e-02,\n",
      "         2.8520e-02,  4.1449e-02,  1.6207e-02, -3.1468e-03,  7.3972e-03,\n",
      "        -3.2111e-02,  3.2255e-03, -3.0692e-02, -1.8224e-02,  8.3117e-03,\n",
      "        -1.5992e-02, -2.9128e-02, -2.1770e-02,  5.3536e-02, -5.4486e-02,\n",
      "         2.2089e-02, -1.3552e-02, -2.4525e-02, -3.1947e-02,  2.7557e-02,\n",
      "        -1.8300e-02, -8.0205e-03, -3.1661e-03,  2.0833e-02,  7.6277e-03,\n",
      "         8.5798e-05, -9.4880e-03, -5.6969e-03,  2.2438e-02, -4.9612e-02,\n",
      "        -1.6423e-02, -3.7413e-02, -2.3648e-02,  2.2182e-02,  1.1625e-02,\n",
      "         7.3036e-03, -1.3317e-02,  2.1622e-03, -5.4788e-03, -3.5196e-02,\n",
      "         1.7532e-02,  3.5754e-02,  2.3786e-02, -1.5152e-02,  7.8752e-03,\n",
      "        -3.9957e-02,  9.7383e-03, -4.0101e-02, -1.8215e-03, -3.5447e-02,\n",
      "        -4.2414e-02,  4.9034e-04,  3.7700e-03,  9.4821e-03, -1.4387e-02,\n",
      "        -3.7766e-02,  9.8579e-03, -3.2587e-02, -3.9242e-02,  2.8729e-02,\n",
      "         3.0664e-02,  6.5137e-03, -1.8806e-02, -2.4924e-02, -1.4181e-02,\n",
      "        -2.8906e-02,  1.6229e-02, -2.0192e-02,  1.3920e-02,  1.5921e-02,\n",
      "        -3.4065e-02,  1.3239e-02,  5.3756e-04, -3.1524e-03,  4.3876e-02,\n",
      "        -6.1507e-03,  6.1911e-03,  2.7418e-02,  2.9885e-02, -1.3887e-03,\n",
      "        -2.7964e-03, -1.0271e-02,  4.2416e-03,  3.3764e-02, -5.4905e-02,\n",
      "         3.3500e-02, -2.3441e-03, -2.2459e-02,  2.1627e-02, -5.0020e-02,\n",
      "         1.8490e-02,  3.9473e-02, -3.4907e-02,  3.1168e-02, -3.3442e-02,\n",
      "        -2.7572e-02,  1.3674e-02, -1.5370e-02, -1.2392e-02, -4.5396e-02,\n",
      "        -2.9191e-02, -5.5716e-02,  1.1282e-02, -3.2459e-02, -5.6017e-02,\n",
      "        -2.6157e-02, -2.9493e-02, -2.1479e-02,  3.4646e-02, -4.0503e-03,\n",
      "        -1.1557e-02,  2.5735e-02,  3.6316e-02,  3.4851e-02, -1.7793e-02,\n",
      "        -4.9240e-02, -3.2419e-02, -4.3779e-02, -2.1338e-02, -1.8803e-02,\n",
      "        -2.6322e-02, -4.4120e-02, -3.0868e-02, -4.3649e-03, -3.1656e-03,\n",
      "        -1.2170e-02,  3.1831e-02,  1.3228e-02,  4.4997e-02, -3.6902e-02,\n",
      "         2.6894e-02, -1.2379e-02,  3.8433e-02, -1.7313e-02, -3.6148e-02,\n",
      "         2.2511e-02, -2.5057e-03, -1.7895e-02, -4.1503e-02, -3.9942e-02,\n",
      "        -1.8160e-03, -2.6989e-02, -4.1242e-02, -2.9215e-02, -2.8418e-02,\n",
      "        -3.1523e-03,  2.5086e-02, -9.3595e-03, -2.8904e-02,  1.8682e-02,\n",
      "        -3.7909e-02, -2.8781e-02, -1.3897e-02, -2.7925e-02, -4.2436e-02,\n",
      "         4.9022e-02,  1.1839e-02,  3.9786e-04, -8.9812e-03,  5.6372e-02,\n",
      "         3.3839e-02, -4.0641e-02,  3.7834e-02, -4.5613e-02, -4.8691e-02,\n",
      "        -2.2422e-02,  4.3084e-02,  2.7976e-02, -2.5304e-02, -5.1466e-02,\n",
      "        -4.4729e-03, -1.0793e-02,  3.7114e-02, -5.4772e-02,  4.0403e-02,\n",
      "         3.2893e-02, -3.5930e-02,  3.8109e-02, -3.2895e-02, -2.6077e-02,\n",
      "        -2.7541e-02, -2.7351e-02, -8.1285e-04, -2.9432e-02,  1.8122e-02,\n",
      "         1.9064e-02, -4.2680e-02, -1.2290e-03,  2.0616e-02, -4.6947e-02,\n",
      "        -4.2650e-02,  2.7686e-02, -1.2635e-02,  1.4488e-02, -8.0727e-03,\n",
      "         3.4260e-02,  9.9979e-03, -1.3727e-02,  2.2922e-03,  6.0369e-03,\n",
      "         5.9670e-02, -5.1076e-03, -4.5336e-02, -5.9754e-02, -3.0517e-02,\n",
      "        -4.2464e-02, -7.1872e-02,  2.2499e-03,  3.1938e-03,  1.0324e-02,\n",
      "        -1.1316e-02, -1.1843e-02, -3.0198e-02,  1.0262e-02, -1.4432e-02,\n",
      "        -6.6637e-03, -1.0187e-02, -4.5328e-02, -1.5360e-02, -3.9335e-02,\n",
      "        -1.7056e-02, -2.2531e-02,  1.8705e-02, -4.6006e-02, -6.0689e-02,\n",
      "         2.9137e-03,  2.4486e-02, -4.2114e-02, -2.4830e-03, -5.2891e-02,\n",
      "        -8.3318e-03,  2.7416e-02, -4.6129e-02, -3.9380e-02, -1.0235e-02,\n",
      "        -4.0867e-02, -2.6436e-02,  2.9964e-02,  3.1704e-03,  2.1421e-02,\n",
      "        -2.0889e-02,  3.4221e-02, -1.0781e-02, -2.2664e-02, -6.1017e-02,\n",
      "        -3.0696e-02, -2.7941e-02, -4.8893e-02,  1.7091e-02, -6.0020e-03,\n",
      "        -3.9835e-02, -2.0492e-02, -2.2849e-02,  1.8593e-02, -3.9362e-02,\n",
      "         2.3700e-02, -4.6391e-03, -5.3185e-03,  1.2966e-02, -2.9805e-02,\n",
      "        -1.0223e-02,  2.2277e-02, -4.3784e-02,  1.9173e-03,  3.7161e-04,\n",
      "         3.0816e-02, -4.3456e-03, -3.7404e-02,  3.7445e-02, -1.8758e-02,\n",
      "         1.3535e-02, -2.0152e-02, -1.1145e-02, -5.8739e-02,  1.5542e-02,\n",
      "         2.4084e-02,  1.9575e-02,  1.6202e-02,  4.9973e-02, -7.0071e-03,\n",
      "        -3.0191e-02, -3.8132e-02, -4.4396e-02, -4.4180e-03, -3.0050e-02,\n",
      "        -3.3862e-02, -4.7201e-02,  7.5045e-03,  3.8330e-02, -3.2235e-02,\n",
      "         3.6641e-05,  9.2408e-03, -1.3027e-02, -4.5728e-02, -5.1296e-02,\n",
      "         5.7702e-02,  2.2304e-02,  2.2785e-02, -2.4622e-02, -2.8324e-02,\n",
      "         7.4239e-03, -7.5109e-03,  2.9951e-02,  1.4235e-02,  3.9695e-02,\n",
      "        -3.2928e-02, -3.3119e-02, -2.3896e-02,  3.8780e-02,  1.4126e-02,\n",
      "        -5.2481e-03,  2.2003e-02, -1.1916e-02, -4.3261e-02,  2.4619e-02,\n",
      "        -4.9242e-02,  2.1084e-02,  1.3856e-02, -2.7857e-02, -2.2570e-02,\n",
      "        -3.2142e-02, -2.7504e-02,  8.7976e-03,  1.1156e-02, -3.4504e-02,\n",
      "        -4.5309e-02,  1.9719e-02, -4.4061e-02, -5.0231e-02,  3.7344e-02,\n",
      "         1.9704e-02, -2.1481e-02, -4.7364e-02, -1.3113e-02, -2.0240e-02,\n",
      "         4.4674e-03, -3.1204e-02,  7.2183e-03, -4.1439e-02, -2.1091e-02,\n",
      "        -2.6877e-02, -8.4217e-03, -3.0170e-03,  2.6329e-02,  1.9694e-02,\n",
      "        -4.3908e-02, -3.3040e-02,  5.0487e-03, -3.2574e-02,  1.7116e-02,\n",
      "         6.2926e-03, -2.4187e-02,  2.1542e-02, -4.6413e-02, -4.3354e-02,\n",
      "        -7.3367e-02,  1.8077e-02, -7.4632e-03,  1.8792e-03,  1.3851e-02,\n",
      "         3.4101e-02,  1.5413e-02,  1.7782e-02,  1.2969e-02,  1.0087e-02,\n",
      "        -1.7763e-03, -4.3309e-02, -2.4202e-02, -1.6117e-02, -2.5968e-02,\n",
      "         1.6622e-02, -1.2862e-02, -3.1764e-04,  2.6044e-02, -3.0907e-02,\n",
      "        -1.9370e-02,  2.3770e-02,  2.9814e-02, -2.1138e-02,  2.1279e-02,\n",
      "        -3.5907e-02,  1.8351e-02, -1.0847e-02, -6.0272e-02,  3.8942e-02,\n",
      "        -2.8716e-03, -2.1176e-02, -2.2023e-02,  1.6335e-02,  4.4041e-02,\n",
      "        -1.3159e-02,  1.4743e-02,  3.2276e-02,  1.3748e-02, -5.6528e-03,\n",
      "        -7.4672e-03,  2.6927e-02,  1.8394e-02,  1.4972e-02,  1.8053e-03,\n",
      "         3.4700e-02, -1.3972e-02,  3.3234e-03,  1.5443e-02, -1.7401e-02,\n",
      "        -1.8149e-02, -5.0599e-02, -3.1348e-02,  4.1075e-02,  2.6803e-02,\n",
      "         2.6070e-02, -3.2397e-02, -2.9797e-02,  1.6585e-02, -1.8460e-02,\n",
      "         1.9925e-03, -2.4648e-02,  5.6343e-03, -3.0020e-02, -1.4953e-02,\n",
      "         1.2867e-02, -1.3229e-02, -8.5949e-03, -2.3932e-02,  5.4372e-03,\n",
      "         2.8579e-03, -2.3148e-02,  2.8078e-02,  1.3592e-02, -2.5256e-02,\n",
      "         3.9180e-02,  1.9725e-02])), ('4.weight', tensor([[ 0.0137, -0.0430, -0.0272,  ..., -0.0181, -0.0026, -0.0185],\n",
      "        [ 0.0056, -0.0071, -0.0439,  ...,  0.0207, -0.0438, -0.0380],\n",
      "        [-0.0026, -0.0476, -0.0580,  ..., -0.0335,  0.0397,  0.0389],\n",
      "        ...,\n",
      "        [ 0.0133, -0.0231,  0.0252,  ..., -0.0117, -0.0290,  0.0237],\n",
      "        [ 0.0006,  0.0348,  0.0012,  ...,  0.0404, -0.0188, -0.0290],\n",
      "        [ 0.0102, -0.0157,  0.0428,  ..., -0.0435, -0.0325, -0.0721]])), ('4.bias', tensor([-4.3124e-02, -6.0502e-02, -9.9909e-03, -2.1852e-02, -1.2212e-02,\n",
      "        -2.9732e-02, -7.6704e-03, -1.2614e-02, -2.3956e-02, -1.9277e-02,\n",
      "         2.9833e-02, -3.0973e-02,  5.5445e-02,  2.1306e-02,  5.6438e-02,\n",
      "        -2.1641e-02,  4.6036e-02, -3.3870e-03,  2.3088e-04,  1.8155e-02,\n",
      "        -2.1638e-02,  1.2251e-03, -3.7239e-02,  7.7374e-03,  6.2584e-02,\n",
      "         1.0854e-02, -1.5807e-02, -4.1262e-02,  2.7087e-02, -1.1643e-02,\n",
      "        -4.7637e-02,  2.0163e-02,  3.3533e-02,  2.4869e-02,  3.4862e-02,\n",
      "        -4.6115e-02,  5.2560e-03, -8.4404e-03,  5.4451e-02, -2.8487e-02,\n",
      "        -8.5966e-03,  1.0529e-02,  3.2453e-02,  2.4697e-02, -5.4063e-02,\n",
      "        -4.0588e-02,  3.4630e-03, -5.3718e-03,  4.8941e-02,  1.9541e-02,\n",
      "        -5.5252e-03,  3.4766e-02, -1.3416e-02,  7.0290e-02, -9.7227e-03,\n",
      "        -5.2023e-02, -4.5597e-02, -2.9911e-03,  2.2905e-02, -4.5266e-02,\n",
      "        -3.8293e-02,  1.9257e-02,  6.1705e-02, -7.2137e-03, -6.4386e-03,\n",
      "        -5.8993e-02, -6.3635e-02, -5.1172e-02, -2.1474e-02,  5.6160e-04,\n",
      "         7.3992e-03,  9.7293e-03, -1.8194e-02,  5.8338e-03,  3.7840e-02,\n",
      "        -1.0026e-02,  2.2472e-02, -2.9799e-03,  3.0015e-02,  2.1913e-02,\n",
      "        -1.6694e-02,  1.4551e-02,  2.2794e-02,  1.2725e-02, -1.2813e-02,\n",
      "         1.4224e-02, -1.9654e-02,  5.1163e-02, -6.6237e-02, -2.2005e-02,\n",
      "        -3.4572e-02,  1.2666e-03,  3.6816e-02, -3.3796e-03, -6.0114e-03,\n",
      "        -2.6951e-02,  2.8808e-02, -1.4202e-02,  1.6017e-02,  1.0514e-02,\n",
      "         4.5350e-02,  2.5642e-02,  2.4090e-02,  4.7057e-02, -1.0526e-02,\n",
      "        -8.7865e-03,  3.8751e-03, -1.4291e-02, -1.2814e-02,  6.4241e-02,\n",
      "        -4.6862e-02, -1.8674e-02,  1.0244e-02, -2.5539e-02, -1.0171e-02,\n",
      "         2.4674e-04,  1.4987e-02, -1.1995e-02,  2.3370e-02,  1.9569e-02,\n",
      "        -2.6636e-02,  5.1960e-03,  2.3475e-02,  2.1167e-02,  2.8923e-03,\n",
      "        -1.1644e-02,  1.1655e-03, -9.1523e-03,  1.9265e-02, -1.4003e-02,\n",
      "        -1.0786e-02, -2.1334e-02, -1.1801e-02,  2.2192e-02, -3.7840e-02,\n",
      "        -5.8401e-02, -3.2528e-03,  1.0508e-03, -2.3902e-02, -1.1851e-04,\n",
      "         2.6648e-02,  1.0436e-02, -6.0071e-02, -1.0809e-02, -3.8385e-02,\n",
      "         2.7039e-03, -2.2861e-02, -2.3970e-03, -2.9505e-02, -1.9569e-02,\n",
      "        -1.5112e-02, -1.1818e-02, -2.3214e-02, -4.4718e-02, -2.0141e-02,\n",
      "         2.0536e-02, -3.0825e-02, -2.9510e-02, -1.6246e-02,  4.2064e-02,\n",
      "         1.1527e-02, -6.3050e-02,  2.2743e-02,  4.2392e-02,  3.8362e-02,\n",
      "        -2.9204e-02, -1.3321e-03,  4.5630e-02,  3.2790e-02,  8.6312e-03,\n",
      "         3.9444e-03, -1.6840e-02,  2.0393e-02,  2.5829e-02, -2.6723e-02,\n",
      "        -2.2201e-02,  1.3232e-02, -6.7439e-02, -2.2364e-02,  5.6992e-03,\n",
      "         1.2293e-02, -7.1308e-03, -2.6152e-02, -4.3574e-03, -3.1458e-02,\n",
      "         5.3002e-02, -1.0658e-02,  4.9902e-02,  7.0980e-03,  2.3503e-02,\n",
      "        -2.9209e-02,  2.2887e-02, -4.3271e-02, -1.0546e-02, -8.5992e-04,\n",
      "        -1.0860e-03, -1.4332e-03,  2.2155e-02,  1.3470e-02,  2.9084e-02,\n",
      "        -2.6335e-02, -2.5792e-02, -6.2941e-02, -5.4506e-02,  2.7839e-02,\n",
      "         5.9264e-02, -4.6552e-03, -3.6333e-02, -2.5720e-02, -3.9555e-02,\n",
      "         3.5746e-02, -5.4661e-02, -7.2238e-03, -2.3533e-02,  5.1847e-03,\n",
      "         6.5663e-02,  2.4837e-03, -3.4101e-02, -1.6710e-02, -4.7090e-03,\n",
      "        -3.5251e-03, -2.6543e-02, -2.4564e-02, -3.9155e-02, -2.1740e-02,\n",
      "         1.9267e-02,  5.7205e-02, -1.6727e-02, -7.2846e-03,  2.7331e-02,\n",
      "        -1.9527e-02,  1.1877e-02,  9.4022e-03, -1.5823e-02, -8.0902e-03,\n",
      "        -1.3314e-03, -4.2347e-02, -2.2915e-02,  5.9318e-02, -1.3526e-02,\n",
      "         5.5669e-02,  2.2128e-02,  4.3442e-02,  4.8556e-02, -3.1538e-03,\n",
      "         2.3287e-02,  7.6887e-03, -3.8528e-02,  3.0603e-02, -1.3844e-03,\n",
      "         2.7112e-02,  1.1429e-02, -3.4682e-02, -1.5918e-02, -3.5867e-02,\n",
      "         3.5861e-02, -1.2086e-02, -2.0581e-02,  2.3238e-02,  5.4454e-02,\n",
      "         2.2233e-02,  6.6475e-02, -2.9853e-02, -5.5693e-03,  4.0315e-02,\n",
      "         1.7253e-02, -4.3912e-02, -2.3705e-02, -3.3213e-02,  3.8856e-03,\n",
      "         1.2916e-02, -4.4598e-04, -6.6086e-02,  3.6583e-03, -2.6494e-03,\n",
      "         7.7681e-03,  1.9576e-02,  3.1406e-02, -4.4384e-02,  1.6857e-02,\n",
      "        -2.8796e-02,  3.4063e-02,  2.4577e-02,  2.0501e-02,  2.2328e-02,\n",
      "         2.7152e-02, -7.2320e-04, -4.6486e-02,  1.0273e-02, -9.6207e-03,\n",
      "        -4.2170e-02, -3.0856e-02,  6.1294e-02, -4.1007e-02, -4.5154e-02,\n",
      "        -3.0413e-02, -3.4624e-02, -7.5980e-03,  1.0514e-03, -3.3193e-02,\n",
      "         1.2548e-02,  4.8595e-03, -9.5899e-05, -1.7307e-02,  3.1489e-02,\n",
      "        -4.6316e-02, -2.0256e-02, -3.1465e-02, -2.3768e-02, -3.2722e-02,\n",
      "        -3.2816e-02, -1.5161e-02,  3.6628e-02, -5.0692e-03, -1.2813e-03,\n",
      "        -3.2450e-04, -9.9619e-04,  2.4994e-02, -3.5756e-02,  5.9012e-02,\n",
      "        -3.9452e-03,  2.4360e-03,  3.7260e-02,  9.5665e-03,  3.8291e-02,\n",
      "        -1.5048e-02,  6.1349e-02, -4.0649e-02,  7.2849e-03,  1.9998e-02,\n",
      "         1.8387e-02, -3.2916e-02,  4.3233e-02, -3.1634e-02, -2.3056e-02,\n",
      "         4.8005e-02, -1.7906e-02,  1.9458e-02, -1.9784e-03,  4.7338e-02,\n",
      "         1.4753e-02, -3.2467e-02, -6.8417e-02,  4.3973e-02,  1.6828e-02,\n",
      "        -2.1364e-02, -2.4663e-02, -5.4697e-02, -1.5644e-02,  8.6422e-03,\n",
      "        -4.4072e-02, -7.2345e-03, -4.2374e-02,  3.1124e-02, -2.2392e-02,\n",
      "        -5.1708e-02,  1.6299e-02,  2.9046e-02,  1.4808e-02,  1.7983e-02,\n",
      "         1.2550e-02, -3.8565e-02,  4.9358e-02, -1.4730e-02,  2.5914e-02,\n",
      "         5.1934e-03, -5.0209e-02, -5.0155e-03,  5.1062e-02, -1.6150e-02,\n",
      "         1.4531e-03,  6.1821e-03,  5.4210e-02, -3.0316e-02, -1.3523e-02,\n",
      "         1.9947e-02, -7.9925e-03,  2.6262e-02, -5.0759e-02,  4.1231e-02,\n",
      "        -1.0973e-02, -1.7466e-02, -3.2100e-02, -1.3387e-03,  3.3065e-02,\n",
      "        -7.6808e-04,  2.1425e-02, -3.2060e-02, -2.5195e-02, -4.2522e-02,\n",
      "        -1.3927e-02,  8.6624e-05, -1.3842e-02,  8.8855e-03,  3.8045e-02,\n",
      "         3.8306e-02, -2.4866e-03,  3.4300e-02, -4.2430e-02,  1.5327e-03,\n",
      "         3.3564e-02,  2.6359e-02,  5.3408e-02, -2.1682e-02,  3.8900e-02,\n",
      "         5.5135e-02,  1.5800e-02, -1.9902e-03,  8.2426e-06, -7.8338e-04,\n",
      "         5.1989e-02,  2.5964e-02, -1.2189e-02, -1.8365e-02,  7.5271e-03,\n",
      "        -3.9939e-02,  3.3704e-02,  2.7504e-02, -2.1811e-02, -5.1286e-03,\n",
      "         2.8058e-02, -7.7349e-03,  1.2280e-02,  5.0344e-02,  6.8264e-02,\n",
      "         8.9642e-03,  4.3033e-03,  4.7700e-03, -6.6330e-02, -4.9886e-02,\n",
      "        -3.3477e-02,  3.3230e-02,  1.5827e-02, -1.2797e-02, -3.0789e-02,\n",
      "        -5.7868e-02,  5.4389e-03,  6.4398e-02, -2.3170e-02,  3.4781e-02,\n",
      "         1.6974e-02, -1.1342e-03, -2.0155e-02,  2.5220e-02, -2.4919e-02,\n",
      "         1.3042e-04,  3.1249e-03,  1.8085e-02, -1.1524e-02,  2.9426e-02,\n",
      "         5.7996e-03, -2.1135e-02,  4.7809e-02,  3.8015e-02, -4.8632e-02,\n",
      "         1.5954e-02,  1.0550e-02,  4.4322e-02, -4.6365e-02, -6.9497e-03,\n",
      "        -1.6789e-02, -4.8698e-02,  2.8175e-02,  3.3122e-02,  9.2632e-03,\n",
      "         2.9624e-02, -1.4307e-03,  4.5015e-02, -4.6675e-03, -1.1916e-02,\n",
      "        -6.9771e-02, -3.9326e-02, -4.3732e-02, -6.9605e-03,  4.6500e-02,\n",
      "        -2.1927e-02,  2.7500e-02,  2.6852e-02, -4.0941e-03,  3.9245e-02,\n",
      "         3.1266e-02,  7.1523e-02, -3.8829e-02, -7.1593e-03,  5.3229e-03,\n",
      "         3.8135e-02, -2.4916e-02,  4.8570e-03, -4.1209e-02, -4.7189e-02,\n",
      "         2.5437e-02, -3.6571e-02,  2.6438e-02, -3.0647e-02, -2.4367e-02,\n",
      "        -1.5899e-02,  4.4824e-02,  1.6655e-02,  3.1160e-02,  2.7343e-02,\n",
      "        -4.3884e-03, -3.1279e-02,  1.8112e-02, -4.1643e-02,  4.7637e-03,\n",
      "         4.0766e-02, -1.7687e-03,  3.8913e-02, -1.4729e-02,  2.4563e-02,\n",
      "        -3.8219e-02, -1.1696e-02])), ('6.weight', tensor([[-0.0043, -0.0165, -0.0277,  ..., -0.0258, -0.0354,  0.0155],\n",
      "        [ 0.0180,  0.0168,  0.0259,  ...,  0.0140, -0.0334, -0.0136],\n",
      "        [ 0.0002, -0.0226,  0.0350,  ...,  0.0316, -0.0015, -0.0047],\n",
      "        ...,\n",
      "        [ 0.0443, -0.0087,  0.0338,  ..., -0.0088,  0.0079, -0.0672],\n",
      "        [ 0.0183, -0.0113,  0.0347,  ...,  0.0212, -0.0281,  0.0089],\n",
      "        [-0.0314, -0.0074, -0.0340,  ..., -0.0404, -0.0402, -0.0257]])), ('6.bias', tensor([-0.0459,  0.0155, -0.0086, -0.0242, -0.0116,  0.0052, -0.0175, -0.0060,\n",
      "        -0.0298,  0.0259, -0.0280, -0.0019,  0.0257, -0.0485,  0.0378, -0.0385,\n",
      "         0.0358,  0.0338,  0.0725,  0.0230, -0.0205, -0.0252,  0.0504,  0.0503,\n",
      "        -0.0290, -0.0201, -0.0024, -0.0441,  0.0623, -0.0089,  0.0261, -0.0357,\n",
      "        -0.0054,  0.0289,  0.0176,  0.0220,  0.0656, -0.0022,  0.0155, -0.0076,\n",
      "         0.0158,  0.0284, -0.0143, -0.0077, -0.0381, -0.0520,  0.0023, -0.0043,\n",
      "        -0.0310, -0.0436,  0.0013,  0.0195,  0.0065, -0.0388,  0.0712,  0.0049,\n",
      "         0.0385, -0.0010,  0.0478,  0.0033, -0.0153,  0.0555,  0.0241, -0.0228,\n",
      "        -0.0481,  0.0147,  0.0149,  0.0110,  0.0072, -0.0011, -0.0491, -0.0051,\n",
      "         0.0411,  0.0446,  0.0189, -0.0121, -0.0133,  0.0559, -0.0386,  0.0342,\n",
      "         0.0528, -0.0382, -0.0002,  0.0142, -0.0222, -0.0541,  0.0689,  0.0146,\n",
      "        -0.0167, -0.0361,  0.0006,  0.0637,  0.0441, -0.0522,  0.0467, -0.0145,\n",
      "         0.0388, -0.0308, -0.0524,  0.0345,  0.0611,  0.0417, -0.0205,  0.0300,\n",
      "         0.0119,  0.0299,  0.0006,  0.0178,  0.0067,  0.0111,  0.0237,  0.0019,\n",
      "         0.0593,  0.0500,  0.0260,  0.0206,  0.0617,  0.0585,  0.0199, -0.0429,\n",
      "         0.0397, -0.0333, -0.0443, -0.0019, -0.0191, -0.0387, -0.0429, -0.0278,\n",
      "         0.0418, -0.0331, -0.0430,  0.0117,  0.0031,  0.0330,  0.0081,  0.0118,\n",
      "         0.0352,  0.0116,  0.0178, -0.0254,  0.0712, -0.0362, -0.0134, -0.0762,\n",
      "         0.0347, -0.0411,  0.0738,  0.0497,  0.0530, -0.0043, -0.0127,  0.0235,\n",
      "        -0.0431, -0.0438,  0.0052, -0.0138,  0.0435, -0.0279,  0.0254, -0.0040,\n",
      "        -0.0032,  0.0551, -0.0426, -0.0115, -0.0102, -0.0077, -0.0208, -0.0122,\n",
      "         0.0746, -0.0142,  0.0452,  0.0248,  0.0090,  0.0243, -0.0095, -0.0123,\n",
      "        -0.0317,  0.0197, -0.0013,  0.0130, -0.0184, -0.0010,  0.0156,  0.0582,\n",
      "        -0.0396,  0.0660,  0.0763, -0.0517,  0.0593, -0.0013, -0.0285, -0.0093,\n",
      "         0.0034, -0.0432,  0.0260,  0.0223, -0.0145, -0.0050,  0.0156, -0.0208,\n",
      "         0.0064,  0.0485,  0.0176, -0.0040, -0.0022, -0.0169, -0.0076,  0.0557,\n",
      "         0.0546,  0.0064,  0.0144,  0.0080,  0.0214, -0.0220,  0.0345,  0.0381,\n",
      "         0.0210, -0.0460,  0.0221,  0.0263, -0.0090,  0.0269,  0.0003,  0.0531,\n",
      "        -0.0027, -0.0082, -0.0260, -0.0385, -0.0483, -0.0305,  0.0075,  0.0536,\n",
      "         0.0137,  0.0732, -0.0063, -0.0299, -0.0086,  0.0516, -0.0488, -0.0410,\n",
      "        -0.0048,  0.0763, -0.0082, -0.0115, -0.0108, -0.0217,  0.0505,  0.0443,\n",
      "         0.0592, -0.0614, -0.0327, -0.0268,  0.0324, -0.0431,  0.0261, -0.0055,\n",
      "         0.0139, -0.0003, -0.0043, -0.0321,  0.0547,  0.0272, -0.0056, -0.0235,\n",
      "         0.0365, -0.0046,  0.0300,  0.0246,  0.0488,  0.0043,  0.0478,  0.0279,\n",
      "         0.0106, -0.0028,  0.0443, -0.0035,  0.0700,  0.0180, -0.0406, -0.0277,\n",
      "        -0.0486, -0.0312, -0.0125,  0.0193,  0.0512, -0.0376,  0.0296, -0.0248,\n",
      "        -0.0147,  0.0676,  0.0164,  0.0081,  0.0132,  0.0083,  0.0390,  0.0377,\n",
      "         0.0356,  0.0038, -0.0346,  0.0257,  0.0754,  0.0084,  0.0251, -0.0110,\n",
      "        -0.0202, -0.0460,  0.0207,  0.0106,  0.0073,  0.0690,  0.0392,  0.0218,\n",
      "         0.0406, -0.0310, -0.0079, -0.0393, -0.0025,  0.0199, -0.0063,  0.0665,\n",
      "        -0.0012,  0.0214, -0.0249, -0.0523,  0.0229, -0.0238,  0.0025, -0.0313,\n",
      "         0.0486, -0.0384, -0.0150,  0.0509, -0.0122, -0.0190,  0.0265, -0.0021,\n",
      "        -0.0438, -0.0056,  0.0440, -0.0459,  0.0136,  0.0558, -0.0461, -0.0104,\n",
      "        -0.0461,  0.0037, -0.0023,  0.0142,  0.0512, -0.0213,  0.0047, -0.0334,\n",
      "        -0.0395,  0.0325, -0.0370,  0.0497,  0.0049,  0.0051,  0.0653,  0.0050,\n",
      "        -0.0020,  0.0147,  0.0356,  0.0256, -0.0487, -0.0458,  0.0587, -0.0547,\n",
      "        -0.0117,  0.0659, -0.0474,  0.0198, -0.0190,  0.0173, -0.0436, -0.0357,\n",
      "        -0.0304, -0.0157, -0.0253,  0.0113,  0.0310, -0.0212,  0.0136, -0.0032,\n",
      "        -0.0136, -0.0142,  0.0275, -0.0044, -0.0412, -0.0069, -0.0090,  0.0382,\n",
      "        -0.0233, -0.0096,  0.0219,  0.0004, -0.0153, -0.0189,  0.0114,  0.0162,\n",
      "        -0.0431, -0.0198, -0.0387,  0.0582,  0.0724,  0.0125, -0.0110, -0.0281,\n",
      "        -0.0254, -0.0204, -0.0035, -0.0358,  0.0065, -0.0064,  0.0213,  0.0614,\n",
      "         0.0233,  0.0246, -0.0235, -0.0446, -0.0040, -0.0316,  0.0694, -0.0060,\n",
      "        -0.0201,  0.0204, -0.0434, -0.0069, -0.0078, -0.0376, -0.0295, -0.0216,\n",
      "        -0.0345,  0.0323,  0.0091, -0.0020, -0.0308, -0.0619,  0.0239,  0.0310,\n",
      "        -0.0143, -0.0487, -0.0254,  0.0206,  0.0706,  0.0379, -0.0229, -0.0239,\n",
      "         0.0116, -0.0188, -0.0158,  0.0094,  0.0093, -0.0171,  0.0246,  0.0510,\n",
      "         0.0084,  0.0569,  0.0586, -0.0420, -0.0166,  0.0521,  0.0092,  0.0269,\n",
      "        -0.0120,  0.0105,  0.0027, -0.0147, -0.0146,  0.0090,  0.0598, -0.0391,\n",
      "         0.0327, -0.0238, -0.0220, -0.0272, -0.0447,  0.0481, -0.0072,  0.0196,\n",
      "         0.0472,  0.0379, -0.0317,  0.0235, -0.0494,  0.0205,  0.0142, -0.0200,\n",
      "         0.0017, -0.0256, -0.0040, -0.0013,  0.0443,  0.0299,  0.0248,  0.0486,\n",
      "        -0.0154,  0.0782,  0.0543,  0.0060, -0.0310, -0.0233, -0.0014,  0.0167,\n",
      "        -0.0308, -0.0088,  0.0329,  0.0451,  0.0328,  0.0513,  0.0386, -0.0291])), ('8.weight', tensor([[-0.0522,  0.0132, -0.0085,  ...,  0.0222,  0.0304,  0.0268],\n",
      "        [-0.0142, -0.0050,  0.0037,  ..., -0.0092, -0.0025, -0.0128],\n",
      "        [ 0.0201,  0.0324,  0.0271,  ...,  0.0200,  0.0272,  0.0315],\n",
      "        ...,\n",
      "        [-0.0113,  0.0099,  0.0325,  ...,  0.0189,  0.0325,  0.0091],\n",
      "        [ 0.0230,  0.0294, -0.0518,  ..., -0.0371, -0.0095, -0.0092],\n",
      "        [ 0.0307, -0.0291, -0.0325,  ...,  0.0195,  0.0335,  0.0280]])), ('8.bias', tensor([-0.0219, -0.0273,  0.0316, -0.0489, -0.0249,  0.0595,  0.0079,  0.0591,\n",
      "        -0.0350,  0.0422, -0.0135, -0.0113,  0.0167, -0.0321, -0.0159, -0.0345,\n",
      "         0.0533, -0.0091,  0.0448,  0.0188,  0.0211, -0.0657,  0.0502,  0.0123,\n",
      "        -0.0050,  0.0555, -0.0154,  0.0564,  0.0676, -0.0572,  0.0713,  0.0521,\n",
      "         0.0601,  0.0403, -0.0099, -0.0251, -0.0337,  0.0297,  0.0199, -0.0539,\n",
      "        -0.0367, -0.0517,  0.0318,  0.0039,  0.0497, -0.0094, -0.0469,  0.0657,\n",
      "         0.0131, -0.0309,  0.0298,  0.0174, -0.0106,  0.0502, -0.0090, -0.0074,\n",
      "         0.0613, -0.0394,  0.0590, -0.0375,  0.0467,  0.0124,  0.0562, -0.0477,\n",
      "        -0.0221,  0.0565, -0.0240,  0.0402,  0.0384, -0.0197, -0.0168, -0.0536,\n",
      "         0.0505,  0.0696, -0.0350,  0.1000,  0.0625,  0.0328, -0.0013,  0.0064,\n",
      "         0.0267, -0.0172,  0.0041, -0.0334, -0.0174,  0.0631, -0.0209,  0.0768,\n",
      "         0.0059,  0.0545, -0.0340,  0.0395,  0.0754,  0.0239,  0.0151,  0.0062,\n",
      "        -0.0348,  0.0186, -0.0127,  0.0358, -0.0333, -0.0476,  0.0592, -0.0449,\n",
      "        -0.0045,  0.0117, -0.0274,  0.0381,  0.0738, -0.0263,  0.0593, -0.0236,\n",
      "        -0.0048,  0.0758, -0.0267, -0.0912,  0.0512, -0.0534, -0.0200,  0.0384,\n",
      "         0.0764, -0.0394,  0.0712,  0.0087, -0.0460,  0.0582, -0.0004,  0.0041,\n",
      "        -0.0357,  0.0602,  0.0227, -0.0418,  0.0327, -0.0021,  0.0527, -0.0366,\n",
      "        -0.0165, -0.0099,  0.0328,  0.0486,  0.0043, -0.0212, -0.0275,  0.0681,\n",
      "        -0.0437, -0.0840,  0.0269, -0.0091, -0.0091,  0.0257, -0.0449, -0.0017,\n",
      "         0.0370, -0.0260, -0.0093,  0.0820, -0.0443,  0.0598,  0.0269,  0.0389,\n",
      "         0.0513,  0.0671,  0.0631,  0.0257,  0.0633, -0.0475,  0.0023, -0.0587,\n",
      "        -0.0416,  0.0490,  0.0010,  0.0318, -0.0492, -0.0197, -0.0156, -0.0021,\n",
      "         0.0254, -0.0716,  0.0402,  0.0454,  0.0275,  0.0040,  0.0629,  0.0430,\n",
      "         0.0616,  0.0601,  0.0174, -0.0283,  0.0602,  0.0420, -0.0499, -0.0797,\n",
      "         0.0404, -0.0305,  0.0445, -0.0338,  0.0443, -0.0278,  0.0323,  0.0061,\n",
      "        -0.0409, -0.0367, -0.0456,  0.0577,  0.0395, -0.0436, -0.0584, -0.0185,\n",
      "        -0.0277,  0.0377,  0.0572,  0.0318,  0.0815,  0.0443,  0.0360,  0.0530,\n",
      "         0.0577,  0.0616, -0.0262,  0.0748, -0.0319,  0.0314,  0.0408, -0.0218,\n",
      "        -0.0454, -0.0496,  0.0167,  0.0383, -0.0445,  0.0147,  0.0753,  0.0564,\n",
      "         0.0353,  0.0476,  0.0277,  0.0540, -0.0099, -0.0219,  0.0548, -0.0291,\n",
      "        -0.0438, -0.0031,  0.0533,  0.0025, -0.0139,  0.0515, -0.0061,  0.0146,\n",
      "        -0.0372, -0.0073,  0.0350, -0.0424, -0.0408, -0.0018,  0.0423, -0.0350])), ('10.weight', tensor([[ 2.3065e-02,  1.0013e-02,  3.3038e-02,  3.5222e-02, -1.1435e-03,\n",
      "          2.7408e-02, -5.3475e-02, -1.7187e-02,  2.2148e-02,  1.6905e-02,\n",
      "          3.4899e-02, -2.4175e-02,  3.1475e-02,  2.9951e-02,  4.4982e-02,\n",
      "         -1.7391e-02,  3.8365e-02, -8.2691e-03,  5.4733e-02, -5.2976e-03,\n",
      "          6.1094e-02,  1.9388e-03,  4.7392e-02,  2.3063e-02, -2.1566e-02,\n",
      "         -3.9830e-02,  3.6964e-02, -3.7866e-02, -3.0928e-02,  3.0892e-02,\n",
      "          5.7511e-02, -4.8332e-02, -2.8889e-02,  3.1107e-02,  1.4457e-03,\n",
      "         -4.1978e-02,  1.5566e-02, -4.8746e-02,  1.5693e-03,  4.6566e-02,\n",
      "         -1.8366e-03,  9.8666e-03, -3.6832e-02, -3.6374e-02,  4.8331e-02,\n",
      "          2.2058e-02,  1.5952e-02, -4.8812e-02, -1.7531e-03, -3.7992e-02,\n",
      "          4.3378e-02,  2.6073e-03,  3.7412e-02,  4.5431e-02,  3.9155e-02,\n",
      "         -8.0447e-03, -5.7614e-02,  4.4596e-03,  7.1105e-02, -7.7052e-03,\n",
      "         -3.6647e-02,  1.2188e-03,  6.4835e-02, -2.5564e-02, -9.3703e-03,\n",
      "          4.0955e-02,  1.1230e-02,  3.8418e-02,  6.4349e-02, -2.3479e-02,\n",
      "         -1.7113e-03,  1.8442e-02,  6.4190e-02,  6.6620e-02, -1.8688e-02,\n",
      "         -5.0372e-02, -6.2659e-02,  1.0091e-03,  3.6559e-02,  9.1947e-03,\n",
      "         -4.2306e-02,  5.6146e-05, -8.2130e-03, -2.8219e-03, -3.2454e-02,\n",
      "          4.6939e-02, -3.3253e-02, -7.4771e-02, -3.7815e-03, -3.6111e-02,\n",
      "          3.4435e-02,  1.4338e-02, -6.2075e-02, -3.1924e-02,  2.0251e-04,\n",
      "         -3.1376e-02, -4.1661e-02,  2.6492e-02,  1.1545e-03, -3.9894e-03,\n",
      "         -4.0783e-02, -2.3076e-02, -1.5819e-02,  1.2782e-02,  4.0523e-02,\n",
      "          6.6902e-02, -5.1750e-02,  2.7210e-02, -2.6858e-02,  1.4686e-03,\n",
      "          6.0607e-03, -5.2741e-03,  8.7589e-03, -5.3028e-02, -3.1305e-02,\n",
      "          5.3783e-02,  2.9594e-02, -8.6878e-04,  3.7768e-02, -1.1499e-02,\n",
      "         -5.0909e-02,  3.5773e-02, -4.6365e-02, -2.2281e-02, -3.6226e-02,\n",
      "          2.0099e-02, -2.1793e-02, -1.5593e-02,  1.6558e-02, -1.2189e-02,\n",
      "         -2.0346e-02, -2.6291e-03,  2.4309e-02, -4.2361e-02,  2.1762e-02,\n",
      "          3.2094e-02, -4.0409e-02, -9.7673e-03, -5.1835e-02,  3.1118e-02,\n",
      "          5.0408e-02,  6.5300e-03, -8.4449e-03,  2.4611e-02, -1.7461e-02,\n",
      "          4.6021e-03,  4.0661e-02, -2.2019e-02,  5.5983e-02, -1.8071e-03,\n",
      "         -8.5779e-04,  2.6984e-03, -4.1028e-03, -3.8794e-03,  1.0130e-02,\n",
      "         -3.1871e-02, -3.6964e-02,  6.2676e-02, -5.7474e-02, -4.8608e-02,\n",
      "         -2.2193e-02, -4.5877e-02, -4.0228e-02,  5.0661e-02,  5.8251e-02,\n",
      "         -8.6807e-03,  8.6130e-03, -1.9779e-02,  2.7930e-02, -7.1535e-03,\n",
      "          1.4421e-02,  4.2639e-04, -5.5635e-02, -5.6671e-05,  1.4053e-02,\n",
      "         -4.4225e-02, -5.6899e-02, -1.7253e-02,  2.9520e-02,  1.0602e-02,\n",
      "          4.1687e-02,  2.8916e-03, -4.1898e-02, -5.0848e-02, -2.5352e-02,\n",
      "          2.4558e-03,  2.3991e-02, -1.0620e-02,  2.8827e-02, -4.0518e-02,\n",
      "          1.0202e-02, -9.8175e-03, -2.9410e-02,  5.7951e-03,  2.0267e-03,\n",
      "          9.7571e-03, -3.8564e-02, -3.8072e-02, -2.8232e-03,  5.7876e-02,\n",
      "          5.0993e-02,  4.2474e-03,  3.0646e-02,  5.4461e-02, -3.7019e-02,\n",
      "          1.4945e-02,  4.7844e-03,  5.4822e-02, -3.4688e-02,  5.1667e-02,\n",
      "          6.5300e-02, -1.1384e-02, -3.7441e-02,  1.0014e-02,  2.7326e-02,\n",
      "         -2.2953e-02,  5.5142e-02, -3.4000e-02, -2.1446e-02, -4.4298e-02,\n",
      "          1.3640e-02,  2.9944e-02, -4.4215e-02, -9.1365e-03, -1.2548e-02,\n",
      "          8.7004e-04, -6.5802e-03, -5.0097e-02,  2.1786e-02,  5.1207e-03,\n",
      "         -7.1678e-02,  2.7976e-02,  4.4690e-04,  2.8718e-02,  3.9001e-03,\n",
      "          1.8025e-02,  1.5575e-02, -1.3993e-02,  2.4768e-02,  4.6163e-02,\n",
      "         -5.0149e-02, -2.8603e-04,  4.9664e-02, -3.2332e-02,  1.2546e-02,\n",
      "          1.0987e-02,  4.8014e-02,  9.1197e-03,  4.4424e-03, -4.2698e-02,\n",
      "          1.2892e-02,  2.2221e-02, -2.2495e-02,  5.5536e-03,  5.6490e-03,\n",
      "          5.2353e-02]])), ('10.bias', tensor([-0.0506]))])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.47it/s]\n"
     ]
    }
   ],
   "source": [
    "new_neunet.set_loaders(train_loader[0], val_loader[0])\n",
    "new_neunet.train(n_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot losses of resumed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = new_neunet.plot_losses()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
